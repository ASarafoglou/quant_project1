---
title: "Individual Differences of Quantifier Interpretation"
author: "Alexandra Sarafoglou"
date: "November 2022"
output: 
  html_document:
    theme: cerulean
    highlight: tango
    code_folding: hide
---

```{r, message=F}
rm(list=ls())
setwd("~/GitProjects/MostQuantifiersHaveManyMeanings/dev")
library("dplyr")
library("MCMCpack")
library("LaplacesDemon")
library("rstan")
library("plotrix")
library(RColorBrewer)
# data preprocessing
library(lme4) 
library(plyr)
library(corrplot)
library(factoextra)
library(psych)
library(MASS)
library(klaR)
library(rrcov)
library(olsrr) #SR
```

## The Task

In this task, participants have to evaluate whether a certain quantifier logically describes a scenario. Participants read a sentence in which a certain scenario is described, for instance, "20% of the gleerbs are fizzda". Afterwards, the participants read the same sentence again, however, the percentage is now replace with a quantifier, that is, "most of the gleerbs are fizzda". The participant needs to decide whether the second statement is true or false. Percentages are drawn from a uniform distribution ensuring that as many trials have more than 50% as have less than 50%. The quantifiers of interest are *None*, *Few*, *Some*, *Fewer than half*, *Many*, *Most* and *More than half*, *All*. The idea is that *More than half* is defined by more than 50% for everyone while *Most* may be more subjective, and it may be more than *More than half* for everyone. Additionally, we may inspect the relationship between *Many* and *More than half*, and also symmetries between mirrored quantifiers.


### Model

Let $i$ indicate participants, $i = 1, \ldots, I$, $j$ indicate the quantifier, $j = 1, \ldots, 5$, and $k$ indicate the trial for each quantifier, $k = 1, \ldots, K_{ij}$.^[Originally, $K_{ij} = 50$. But this value may be reduced after cleaning on the trial level.] Then $Y_{ijk}$ is the $i$th participant's response to the $j$th quantifier in the $k$th trial, and $Y_{ijk} = 1$ if participants indicate *true*, and $Y_{ijk} = 0$ if participant indicate *false*. Then, we may model $Y_{ijk}$ as a Bernoulli, using the logit link function on the probabilities:

\begin{align*}
Y_{ijk} &\sim \mbox{Bernoulli}(\pi_{ijk}),\\
\pi_{ijk} &= \gamma_{ij} + (1 - 2 \gamma_{ij}) \text{logit}^{-1}(\mu_{ijk}),\\
\end{align*}

$\pi_{ijk} = \gamma_{ij} + (1 - 2 \gamma_{ij}) \text{logit}^{-1}(\mu_{ijk})$

where the second line maps the probability space of $\pi$ onto the real space of $\mu$. There is an additional parameter in there, $\gamma_{ij}$, which is the probability of making a response error on either side (erroneously saying true, or erroneously saying false). Note that each person-quantifier combination has its own response error parameter (which in turn indicates that the response error carries semantic information). We may now place a linear model on $\mu_{ijk}$:

$\mu_{ijk} = \frac{c_{ijk} - \beta_{ij}}{\alpha_{ij}},$

where $c_{ijk}$ indicates the centered percentage, parameters $\beta_{ij}$ indicate the threshold, and parameters $\alpha_{ij}$ correspond to the vagueness of the quantifier. For now, I will place the following priors:

\begin{align*}
\gamma_{ij} &\sim \mbox{Beta}(2, 20),\\
\beta_{ij} &\sim \mbox{Normal}(\delta_j, \sigma_j^2),\\
\alpha_{ij} &\sim \mbox{log-Normal}(\nu_j, \sigma_{\alpha, j}^2),\\
\nu_j &\sim \mbox{Normal}(0, 5^2),\\
\sigma_{\alpha, j}^2 &\sim \mbox{Inverse-Gamma}(2, .2).\\
\delta_j &\sim \mbox{Normal}(0, 5^2).\\
\sigma^2_j &\sim \mbox{Inverse-Gamma}(2, .2).\\
\end{align*}


**Notes Julia:** These priors should be fairly wide and uninformative. Actually, the priors on $\nu$ and $\delta$ might be a bit too wide for a logit model. We might want to adjust these to standard normals.

$response = \frac{Asymptote}{1+\exp{(midpoint-proportion)/scale)}$

## Study 1 {.tabset .tabset-fade}

```{r read-data}
dat <- read.csv("data/exp1-replication-trials.csv")
#View(dat)
head(dat)
nrow(dat)
length(unique(dat$workerid))

# exclude some participants to make the model run quicker
runQuicker <- TRUE
if(runQuicker){
  excludeForSpeed <- sample(unique(dat$workerid), 40)
  dat             <- subset(dat, !(workerid %in% excludeForSpeed))
}
```


### Data Pre-Processing

- We flipped the true and false responses for few and fewer than half
- We removed the quantifiers "all", "none", and "some" from the analysis

```{r preprocessing}
dat$quant <- gsub('\"', '', dat$quant)
# dat       <- subset(dat, quant %in% c('Few', 'Fewer than half', 'Many', 
#                                       'More than half', 'Most'))
dat       <- subset(dat, quant %in% c('Fewer than half', 'More than half'))
quant_pos <- c('Many', 'More than half', 'Most')
quant_neg <- c('Few', 'Fewer than half')

# dat[dat$quant %in% quant_pos,]$response <- ifelse(dat[dat$quant %in% quant_pos,]$response=="true",1,0)
# dat[dat$quant %in% quant_neg,]$response <- ifelse(dat[dat$quant %in% quant_neg,]$response=="true",0, 1)
# dat$response <- as.numeric(dat$response)

dat <- dat %>% mutate(percent_cat=cut(percent, breaks=c(0, 20, 40, 60, 80, 100), 
                  labels=c("1-20","21-40","41-60", "61-80", "81-100")))
```

### Exclusion criteria - Participants

- We excluded 11 participants who had 50% or more reaction times faster than 300ms
- We excluded 7 participants who failed to obey the monotonicity of quantifiers, that is,
for positive quantifiers (many, most, more than half) we expected the probability of providing the true response to increase with increasing proportion. 
The opposite effect should hold for negative quantifiers. 
To apply the monotonicity criterion, we fitted the generalized linear model
to participants' response data with the proportion as a predictor and with 
by-subject random intercept and slope for proportion. 
We excluded participants, who had a negative slope for positive
quantifiers or a positive slope for negative quantifiers. 
- We excluded 1 participant who tool part in a similar experiment. 

```{r exclude-participants1}
# 0,1,5,39,18,32,49,52,53,58,62,81,82,83,85,86,87,76
#exclude <- c(0, 1 , 5, 18, 32, 39, 49, 52, 53, 58, 62, 81, 82, 83, 85, 86, 87, 27, 76)

# Exclusion 1: 50% or more reaction times (read and decide time) are faster than 300ms
dat$rt_too_quick <- dat$read_and_decide_time < 300
excl1_tab <- aggregate(dat$rt_too_quick ~ dat$workerid, FUN = mean)
exclude   <- excl1_tab$`dat$workerid`[excl1_tab$`dat$rt_too_quick` > 0.5]
# 0 32 49 52 53 58 62 81 82 85 87
length(excl1_tab$`dat$workerid`[excl1_tab$`dat$rt_too_quick` > 0.5]) # n = 11
dat <- subset(dat, !(workerid %in% exclude))
```

```{r exclude-participants2}
# Exlusion 2: participants who failed to obey the monotonicity of quantifiers
exclude2 <- c(27, 76)
dat <- subset(dat, !(workerid %in% exclude2))
# subjs        <- unique(dat$workerid)
# quantifiers  <- unique(dat$quant)
# 
# exclusions <- data.frame(matrix(nrow = length(subjs), ncol = length(quantifiers))) 
# colnames(exclusions) <- quantifiers
# rownames(exclusions) <- subjs
#   
# for(i in 1:length(subjs)){
#   
#   for(j in 1:length(quantifiers)){
#     
#   ind_dat <- dat[dat$workerid == subjs[i] & dat$quant == quantifiers[j],]
# 
#     
#    # is the probability to say "true" increasing as the percentage increases?
#    # note: negative quantifiers have been reverse coded
#    exclusions[i, j] <- !is.unsorted(aggregate(response ~ percent_cat, data = ind_dat, FUN = mean)$response)
#     
#   }
#   
# }
```

```{r exclude-participants3}
# Exclusion 3: participated in a similar experiment
# cannot be inferred from this data
```


### Exclusion criteria - Trials

- We excluded trials with response times shorter than 300ms 
- We excluded trials longer than 2500ms

```{r exclude-trials}
## Trial-level
dat <- subset(dat, read_and_decide_time > 300)  # exclude trials that are faster than 300ms
dat <- subset(dat, read_and_decide_time < 2500) # exclude trials that are slower than 2.5s
nrow(dat)
```

# Descriptives

```{r further-preprocessing}
table(dat$quant, dat$workerid)[, 1]
hist(dat$percent) # should be approximately uniform
table(dat$percent>50) # should be approximately half-half
table(dat$response, dat$quant)
dat$qq <- factor(dat$quant)
# 1="All", 2="Few", 3="Fewer than half", 4="Many", 
# 5="More than half", 6="Most", 7="None"; 8="Some"
prop <- tapply(as.numeric(factor(dat$response)) - 1, list(dat$percent, dat$qq), mean, na.rm = T)
# write.csv(dat, "data/exp1-preprocessed.csv", row.names = FALSE)
```

## Descriptive plot, based on observed responses 

```{r fig-quant}
qcols <- brewer.pal(max(length(levels(dat$qq)), 3), "Dark2")[1:length(levels(dat$qq))]

matplot(as.numeric(rownames(prop)), prop
        , pch = 19, col = qcols, type='l', lwd = 2
        , xlab = "Percent", ylab = "Proportion 'true' responses"
        , frame.plot = F)
abline(v = 50, lwd = 1.5, col = "darkgrey")
abline(h = .50, lwd = 1.5, col = "darkgrey")
legend(75, .7, legend = levels(dat$qq), fill = qcols, bg = "white", box.col = "white")
```

I now recode the responses to correspond to the expected direction of response. I will therefore flip TRUE and FALSE responses for the quantifiers *few* and *fewer than half*.

```{r}
dat$resp <- case_when(
  dat$qq %in% quant_pos ~ as.numeric(factor(dat$response)) - 1,
  dat$qq %in% quant_neg ~ -as.numeric(factor(dat$response)) + 2
)
```

```{r fig-quant-2}
prop <- tapply(dat$resp, list(dat$percent, dat$qq), mean, na.rm = T)
matplot(as.numeric(rownames(prop)), prop
        , pch = 19, col = qcols, type='l', lwd = 2
        , xlab = "Percent", ylab = "Proportion 'true' responses"
        , frame.plot = F)
abline(v = 50, lwd = 1.5, col = "darkgrey")
abline(h = .50, lwd = 1.5, col = "darkgrey")
legend(75, .7, legend = levels(dat$qq), fill = qcols, bg = "white", box.col = "white")
```

#### Priors

```{r visualize-priors}
x     <- seq(0, 5, .01)
ysig  <- dlnorm(x, .1, .5)
ysig2 <- 2 * x * dinvgamma(x^2, 2, .1)
plot(x, ysig, type = "l")
lines(x, ysig2, col = 2)
ydelt <- rnorm(x, 0, .1)

M <- 10000
ysig2 <- rinvgamma(M, 2, .1)
ydelt <- rnorm(M, 0, .1)

prioreff <- rnorm(M, ydelt, sqrt(ysig2))

layout(matrix(1:4, ncol = 2, byrow = T))
hist(prioreff)
hist(ydelt)
hist(pnorm(prioreff), xlim = c(0, 1))
hist(pnorm(ydelt), xlim = c(0, 1))
```

#### Model 1: original model (fewer than half/more than half)

```{stan output.var= 'logmod', eval = TRUE}
data {
  int<lower=1> D;                     // # Dimensions of the model: nr. of quantifiers
  int<lower=0> N;                     // # Observations: rows in dataset
  int<lower=1> I;                     // # Participants
  int<lower=0,upper=1> y[N];          // Data 0,1
  vector[N] cperc;                    // Centered Percentages
  int<lower=1,upper=I> sub[N];        // participant vector
  int<lower=0,upper=1> fewer[N];      // Fewer than half
  int<lower=0,upper=1> more[N];       // More than half
}

parameters {
  real delta[D];                       // Means of betas
  real<lower=0> sigma2[D];             // variance of betas
  vector[D] beta[I];                   // vectors of betas
  real nu[D];                          // Means of alphas
  real<lower=0> sigma2alpha[D];        // variance of alphas
  vector<lower=0>[D] alpha[I];         // vectors of alphas
  vector<lower=0,upper=1>[D] gamma[I]; //vector of gammas
  // Prior predictives
  real deltaprior[D];                  // Means of betas
  real<lower=0> sigma2prior[D];        // variance of betas
  vector[D] betaprior[I];              // vectors of betas
  real nuprior[D];                     // Means of alphas
  real<lower=0> sigma2alphaprior[D];   // variance of alphas
  vector<lower=0>[D] alphaprior[I];    // vectors of alphas
  vector<lower=0,upper=1>[D] gammaprior[I]; //vector of gammas
}

transformed parameters {
  real<lower=0> sigma[D];
  real<lower=0> sigmaalpha[D];
  real<lower=0> sigmaprior[D];
  real<lower=0> sigmaalphaprior[D];
  sigma = sqrt(sigma2);
  sigmaalpha = sqrt(sigma2alpha);
  // Prior predictives
  sigmaprior = sqrt(sigma2prior);
  sigmaalphaprior = sqrt(sigma2alphaprior);
}

model {
  vector[N] mu;
  vector[N] p;
  delta       ~ normal(0, 5);
  sigma2      ~ inv_gamma(2, .2);
  nu          ~ normal(0, 5);
  sigma2alpha ~ inv_gamma(2, .2);
  
  for (i in 1:I)
    beta[i] ~ normal(delta, sigma);
  for (i in 1:I)
    alpha[i] ~ lognormal(nu, sigmaalpha);
  for (i in 1:I)
    gamma[i] ~ beta(2, 20);
  for (n in 1:N)
    mu[n] = fewer[n] * (cperc[n] - beta[sub[n], 1]) / alpha[sub[n], 1] + 
            more[n] * (cperc[n] - beta[sub[n], 2]) / alpha[sub[n], 2];
  for (n in 1:N)
    p[n] = fewer[n] * gamma[sub[n], 1] + 
    more[n] * gamma[sub[n], 2] + 
    (1 - 2 * (fewer[n] * gamma[sub[n], 1] + 
    more[n] * gamma[sub[n], 2])) *
  inv_logit(mu[n]);
  y ~ bernoulli(p);
  
  // Prior predictives
  deltaprior       ~ normal(0, 5);
  sigma2prior      ~ inv_gamma(2, .2);
  nuprior          ~ normal(0, 5);
  sigma2alphaprior ~ inv_gamma(2, .2);
  
  for (i in 1:I)
    betaprior[i] ~ normal(deltaprior, sigmaprior);
  for (i in 1:I)
    alphaprior[i] ~ lognormal(nuprior, sigmaalphaprior);
  for (i in 1:I)
    gammaprior[i] ~ beta(2, 20);
}

generated quantities {
  vector[N] mu_pred;
  vector[N] p_pred;
  int<lower=0,upper=1> y_pred[N];
  
  // Prior Predictive
  for (n in 1:N)
  mu_pred[n] = fewer[n] * (cperc[n] - betaprior[sub[n], 1]) / alphaprior[sub[n], 1] + 
               more[n] * (cperc[n] - betaprior[sub[n], 2]) / alphaprior[sub[n], 2];
  for (n in 1:N)
    p_pred[n] = fewer[n] * gammaprior[sub[n], 1] + 
    more[n] * gammaprior[sub[n], 2] + 
    (1 - 2 * (fewer[n] * gammaprior[sub[n], 1] + 
    more[n] * gammaprior[sub[n], 2])) *
    inv_logit(mu_pred[n]);
   y_pred =  bernoulli_rng(p_pred);
}
```

```{stan output.var= 'logmod_adjusted', eval = FALSE}
data {
  int<lower=1> D;                     // # Dimensions of the model: nr. of quantifiers
  int<lower=0> N;                     // # Observations: rows in dataset
  int<lower=1> I;                     // # Participants
  int<lower=0,upper=1> y[N];          // Data 0,1
  vector[N] cperc;                    // Centered Percentages
  int<lower=1,upper=I> sub[N];        // participant vector
  int<lower=0,upper=1> fewer[N];      // Fewer than half
  int<lower=0,upper=1> more[N];       // More than half
}

parameters {
  real delta[D];                       // Means of betas
  real<lower=0> sigma2[D];             // variance of betas
  vector[D] beta[I];                   // vectors of betas
  real nu[D];                          // Means of alphas
  real<lower=0> sigma2alpha[D];        // variance of alphas
  vector<lower=0>[D] alpha[I];         // vectors of alphas
  vector<lower=0,upper=1>[D] gamma[I]; //vector of gammas
  // Prior predictives
  real deltaprior[D];                  // Means of betas
  real<lower=0> sigma2prior[D];        // variance of betas
  vector[D] betaprior[I];              // vectors of betas
  real nuprior[D];                     // Means of alphas
  real<lower=0> sigma2alphaprior[D];   // variance of alphas
  vector<lower=0>[D] alphaprior[I];    // vectors of alphas
  vector<lower=0,upper=1>[D] gammaprior[I]; //vector of gammas
}

transformed parameters {
  real<lower=0> sigma[D];
  real<lower=0> sigmaalpha[D];
  real<lower=0> sigmaprior[D];
  real<lower=0> sigmaalphaprior[D];
  sigma = sqrt(sigma2);
  sigmaalpha = sqrt(sigma2alpha);
  // Prior predictives
  sigmaprior = sqrt(sigma2prior);
  sigmaalphaprior = sqrt(sigma2alphaprior);
}

model {
  vector[N] mu;
  vector[N] p;
  delta       ~ normal(0, 5);
  sigma2      ~ inv_gamma(2, .2);
  nu          ~ normal(-4, 1);
  sigma2alpha ~ inv_gamma(1, .5);
  
  for (i in 1:I)
    beta[i] ~ normal(delta, sigma);
  for (i in 1:I)
    alpha[i] ~ lognormal(nu, sigmaalpha);
  for (i in 1:I)
    gamma[i] ~ beta(2, 20);
  for (n in 1:N)
    mu[n] = fewer[n] * (cperc[n] - beta[sub[n], 1]) / alpha[sub[n], 1] + 
            more[n] * (cperc[n] - beta[sub[n], 2]) / alpha[sub[n], 2];
  for (n in 1:N)
    p[n] = fewer[n] * gamma[sub[n], 1] + 
    more[n] * gamma[sub[n], 2] + 
    (1 - 2 * (fewer[n] * gamma[sub[n], 1] + 
    more[n] * gamma[sub[n], 2])) *
  inv_logit(mu[n]);
  y ~ bernoulli(p);
  
  // Prior predictives
  deltaprior       ~ normal(0, 5);
  sigma2prior      ~ inv_gamma(2, .2);
  nuprior          ~ normal(-4, 1);
  sigma2alphaprior ~ inv_gamma(1, .5);
  for (i in 1:I)
    betaprior[i] ~ normal(deltaprior, sigmaprior);
  for (i in 1:I)
    alphaprior[i] ~ lognormal(nuprior, sigmaalphaprior);
  for (i in 1:I)
    gammaprior[i] ~ beta(2, 20);
}

generated quantities {
  vector[N] mu_pred;
  vector[N] p_pred;
  int y_pred[N];
  
  // Prior Predictive
  for (n in 1:N)
  mu_pred[n] = fewer[n] * (cperc[n] - betaprior[sub[n], 1]) / alphaprior[sub[n], 1] + 
          more[n] * (cperc[n] - betaprior[sub[n], 2]) / alphaprior[sub[n], 2];
            
  for (n in 1:N)
    p_pred[n] = fewer[n] * gammaprior[sub[n], 1] + 
    more[n] * gammaprior[sub[n], 2] + 
    (1 - 2 * (fewer[n] * gammaprior[sub[n], 1] + 
    more[n] * gammaprior[sub[n], 2])) *
  inv_logit(mu_pred[n]);
  y_pred[n] =  bernoulli_rng(p_pred[n]);
}
```

```{r init, cache = TRUE}
getDat <- function(dat){
  # dat <- subset(dat, qq %in% 4:6)
  D <- length(levels(dat$qq))
  N <- nrow(dat)
  I <- length(unique(dat$workerid))
  
  y <- dat$resp
  cperc <- (dat$percent - 50) / 100
  sub <- as.numeric(factor(dat$workerid))
  # identify which trial concerns which quantifier
  # 1="All", 2="Few", 3="Fewer than half", 4="Many", 
  # 5="More than half", 6="Most", 7="None"; 8="Some"
  fewer <- ifelse(dat$qq == 'Fewer than half', 1, 0)
  more  <- ifelse(dat$qq == 'More than half', 1, 0)
  # many  <- ifelse(dat$qq == 'Many', 1, 0)
  # few   <- ifelse(dat$qq == 'Few', 1, 0)
  # most  <- ifelse(dat$qq == 'Most', 1, 0)
  # above <- as.numeric(cperc > 0)
  
  # res <- glm(y ~ cperc + few + fewer + many + more + most, family = binomial(link = "probit"), 
  #   data = dat)
  
  standat <- list(D = D, N = N, I = I, 
                  y = y, 
                  cperc = cperc, sub = sub,
                  #few = few, many = many, most = most, above = above,
                  fewer = fewer, more = more
                  )  
  return(standat)
}

myRunner <- function(standat, iter = 1000, warmup = 400, mod, nchains = 4, ...){
  inits <- function(...){
    list(  delta       = runif(standat$D, -.5, .5)
         , sigma2      = runif(standat$D, 1, 1.5)
         , beta        = matrix(runif(standat$I * standat$D, -1, 1), nrow = standat$I, ncol = standat$D)
         , nu          = runif(standat$D, .1, 1)
         , sigma2alpha = runif(standat$D, 1, 1.5)
         , alpha       = matrix(runif(standat$I * standat$D, .1, 1), nrow = standat$I, ncol = standat$D)
         , deltaprior       = runif(standat$D, -.5, .5)
         , sigma2prior      = runif(standat$D, 1, 1.5)
         , betaprior        = matrix(runif(standat$I * standat$D, -1, 1), nrow = standat$I, ncol = standat$D)
         , nuprior          = runif(standat$D, .1, 1)
         , sigma2alphaprior = runif(standat$D, 1, 1.5)
         , alphaprior       = matrix(runif(standat$I * standat$D, .1, 1), nrow = standat$I, ncol = standat$D))
  }
  # inits <- list(c1 = inits)
  fit <- sampling(mod, verbose=T,
                  data   = standat, 
                  iter   = iter,
                  warmup = warmup,
                  chains = nchains,
                  cores  = nchains,
                  # init = lapply(1:4, inits),
                  pars   = c("delta", "nu", "beta", "alpha", "gamma", "sigma2", "sigma2alpha", 
                             "deltaprior", "nuprior", "betaprior", "alphaprior", "gammaprior", 
                             "sigma2prior", "sigma2alphaprior", "y_pred", "mu_pred", "p_pred")
                  , ...)
  return(fit)}

predat <- getDat(dat)
```

```{r run-logmod, cache = TRUE, eval = FALSE, results = "hide"}
rerun <- T
logmodfit <- myRunner(predat
                      , iter = 3500
                      , warmup = 750
                      , mod = logmod
                      , control = list(adapt_delta = .97, max_treedepth = 14)
                      , nchains = 2)
#save(logmodfit, file = "outstudy1e.rda")
save(logmodfit, file = "out_prior_predictions.rda")
```

```{r run-logmod-adjusted, cache = TRUE, eval = TRUE, results = "hide"}
rerun <- T
logmodfit <- myRunner(predat
                      , iter = 3500
                      , warmup = 750
                      , mod = logmod_adjusted
                      , control = list(adapt_delta = .97, max_treedepth = 14)
                      , nchains = 2)
save(logmodfit, file = "out_prior_predictions_adjusted.rda")
```


#### Model 1: original model (all quantifiers)

```{stan output.var= 'logmod', cache= TRUE, eval = FALSE}
data {
  int<lower=1> D;                     // # Dimensions of the model: nr. of quantifiers
  int<lower=0> N;                     // # Observations: rows in dataset
  int<lower=1> I;                     // # Participants
  int<lower=0,upper=1> y[N];          // Data 0,1 # participant responses
  vector[N] cperc;                    // Centered Percentages
  int<lower=1,upper=I> sub[N];        // participant vector
  // int<lower=0,upper=1> few[N];        // Few
  int<lower=0,upper=1> fewer[N];      // Fewer than half
  // int<lower=0,upper=1> many[N];       // Many
  int<lower=0,upper=1> more[N];       // More than half
  // int<lower=0,upper=1> most[N];       // Most
  int<lower=0,upper=1> above[N];      // Above 50 percent?
}

parameters {
  real delta[D];                       // Means of betas
  real<lower=0> sigma2[D];             // variance of betas
  vector[D] beta[I];                   // vectors of betas
  real nu[D];                          // Means of alphas
  real<lower=0> sigma2alpha[D];        // variance of alphas
  vector<lower=0>[D] alpha[I];         // vectors of alphas
  vector<lower=0,upper=1>[D] gamma[I]; //vector of gammas
}

transformed parameters {
  real<lower=0> sigma[D];
  real<lower=0> sigmaalpha[D];
  sigma = sqrt(sigma2);
  sigmaalpha = sqrt(sigma2alpha);
}

model {
  vector[N] mu;
  vector[N] p;
  delta       ~ normal(0, 5);
  sigma2      ~ inv_gamma(2, .2);
  nu          ~ normal(0, 5);
  sigma2alpha ~ inv_gamma(2, .2);
  
  for (i in 1:I)
    beta[i] ~ normal(delta, sigma);
    
  for (i in 1:I)
    alpha[i] ~ lognormal(nu, sigmaalpha);
    
  for (i in 1:I)
    gamma[i] ~ beta(2, 20);
    
  for (n in 1:N)
    mu[n] = fewer[n] * (cperc[n] - beta[sub[n], 1]) / alpha[sub[n], 1] + 
            //few[n] * (cperc[n] - beta[sub[n], 1]) / alpha[sub[n], 1] + 
            //many[n] * (cperc[n] - beta[sub[n], 3]) / alpha[sub[n], 3] +
            //most[n] * (cperc[n] - beta[sub[n], 5]) / alpha[sub[n], 5]+
            more[n] * (cperc[n] - beta[sub[n], 2]) / alpha[sub[n], 2];
            
  for (n in 1:N)
    p[n] = fewer[n] * gamma[sub[n], 1] + 
    // few[n] * gamma[sub[n], 1] +
    // many[n] * gamma[sub[n], 3] + 
    // most[n] * gamma[sub[n], 5] + 
    more[n] * gamma[sub[n], 2] + 
    (1 - 2 * (fewer[n] * gamma[sub[n], 1] + 
   // few[n] * gamma[sub[n], 1] + 
   //  many[n] * gamma[sub[n], 3] + 
   //  most[n] * gamma[sub[n], 5] * 
    more[n] * gamma[sub[n], 2])) 
  inv_logit(mu[n]);
  y ~ bernoulli(p);
}
```



#### Prior predictives: original model (fewer than half/more than half)

```{r, cache = TRUE}
#load("outstudy1e.rda")
load("out_prior_predictions.rda")
```

- Here is an initial check how well the chains mixed. This is a bit of a problem for the model. I may have to reparameterize to set one quantifier (more than half?) as default.

```{r check_convergence}
hist(summary(logmodfit)$summary[,"Rhat"]
     , breaks = 100
     , main = ""
     , xlab =  "Rhat")

hist(summary(logmodfit)$summary[,"n_eff"]
     , breaks = 100
     , main = ""
     , xlab =  "Number of effective samples")
```

What do we want to look at?
- Prior predictives per person for the threshold, vagueness, and response error
- Data pattern (if available)

```{r inspect_parameters}
# p_pred (probability to give a 'true' response)
# mu_pred (parameter for logisitc link)
# y_pred (0 & 1 responses)

list_of_draws <- extract(logmodfit)
print(names(list_of_draws))
dim(list_of_draws$mu_pred)
dim(list_of_draws$alpha)

for(i in 1:N){
  round(quantile(list_of_draws$alpha[,i , ], c(0.025, 0.5, 0.975)), 4)
}

```

```{r inspect_data_for_each_person}
# participant 1: less than half
# identify which rows in the data correspond to this participant
# identify which percentages were shown
# extract relevant prior predictive data
# plot it
predResponse <- function(datid, quantifier = 'Fewer than half', dat0=dat, draws=list_of_draws){
  
  worker  <- unique(dat0$workerid)[datid]
  person  <- dat0$workerid == worker & dat0$quant == quantifier
  percent <- dat0[person, 'percent']
  pred    <- colMeans(draws$y_pred[, person])

  return(plot(percent, pred,
              las = 1,
              ylim = c(0.40, 0.60),
              xlab = 'Percent',
              ylab = 'p',
              main = paste(quantifier, worker)))
}

par(mfrow = c(2, 3))
N <- 20
for(i in 1:N){
  predResponse(i, quantifier = 'Fewer than half')
}
for(i in 1:N){
  predResponse(i, quantifier = 'More than half')
}
```

- Very flat logistic curves, which is an indication of the alpha parameter being too big
- Fewer than half and more than half slightly differ in their probabilities (bias towards saying true for more than half)
- It would be interesting to see the alpha parameters directly



