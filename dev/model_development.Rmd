---
title: "model_development"
author: "AlexandraSarafoglou"
date: "2023-03-28"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r load-packages-and-functions}
rm(list=ls())
library("rstan")
library("mvtnorm") 
library("papaja")
library("dplyr")
library("magrittr")
library("LaplacesDemon")
library("RColorBrewer")

# helper functions
source('../helper_functions/helpers.R')
source('../helper_functions/init_and_run_short.R')

c1 <- transparentColor(color = "#B3DE69", percent = 50, name = 'colprior')
c2 <- transparentColor(color = "#FCCDE5", percent = 50, name = 'colpost')
c3 <- transparentColor(color = "grey36", percent = 50, name = 'colpost')
```

```{r read-data}
dat_t1 <- read.csv('../data/simulated-all-data-1.csv', header = TRUE)
dat_t2 <- read.csv('../data/simulated-all-data-2.csv', header = TRUE)
# dat_t1 <- read.csv('../data/simulated-all-large-data-1.csv', header = TRUE)
# dat_t2 <- read.csv('../data/simulated-all-large-data-2.csv', header = TRUE)
dat_t1$qq <- as.factor(dat_t1$qq)
dat_t2$qq <- as.factor(dat_t2$qq)
niter      <- 5000
nwarmup    <- 1000
nchains    <- 2
addparams  <- NULL
```

# Model development

## Run the samplers 

The model development is primarily based on sampling success and theoretical adjustments to the model.
Run all stan models.

```{r set-inits-devel, eval = TRUE}
run_all    <- FALSE
prior      <- FALSE
modelname  <- 'mod_partial_exp_t1'

if(run_all){
  
  filename <- list.files('../models/')
  if(prior){
    prior    <- grepl('_prior', filename)
  } else {
    filename <- filename[!grepl('_prior', filename)]
    prior    <- grepl('_prior', filename)
  }
  
  
} else {
  
 filename <- ifelse(prior,  
                    paste0(modelname, '_prior.stan'), 
                    paste0(modelname, '.stan'))
  
}
```

```{r run-models-inits, eval = TRUE}
model_list <- list()

for(i in 1:length(filename)){
#for(i in 1:2){
  
  model_list[[i]] <- rstan::stan_model(file = paste0('../models/', filename[i]))
  
}

```

# Generated Quantities in R 

```{r run-model-in-R, eval = FALSE}
# To-Do: predict data more fine-grained: function which evaluates quantifiers at 
# proportion
# 1 completely random simulated dataset
# 1 with fixed group-level values (based on median) and random individual effects
#dat_t1    <- read.csv('../data/exp1-preprocessed-all.csv', header = TRUE)
dat_t1    <- read.csv('../data/simulated-all-large-data-1.csv', header = TRUE)
dat_t2    <- dat_t1
dat_t1$qq <- as.factor(dat_t1$qq)
dat_t2$qq <- as.factor(dat_t2$qq)

nsims  <- 1
t      <- 2 
predat <- getDat(dat_t1, dat_t2, prior = prior)
predat$D <- 5
sub_t1 <- predat$sub_t1
sub_t2 <- predat$sub_t2
# timepoint 1
mu_pred_t1 <- matrix(NA, nrow = predat$N_t1, ncol = nsims)
p_pred_t1  <- matrix(NA, nrow = predat$N_t1, ncol = nsims)
y_pred_t1  <- matrix(NA, nrow = predat$N_t1, ncol = nsims)
# timepoint 2
mu_pred_t2 <- matrix(NA, nrow = predat$N_t2, ncol = nsims)
p_pred_t2  <- matrix(NA, nrow = predat$N_t2, ncol = nsims)
y_pred_t2  <- matrix(NA, nrow = predat$N_t2, ncol = nsims)

for(s in 1:nsims){
  
  # group-level means:
  muprior_b_more  <- my_rnorm(2, mu = 0   , sigma = 0.2, lower = -0.5, upper = 0.5) # favors values = 50%
  muprior_b_fewer <- my_rnorm(2, mu = 0   , sigma = 0.2, lower = -0.5, upper = 0.5) # favors values = 50%
  muprior_b_most  <- my_rnorm(2, mu = 0.3 , sigma = 0.2, lower = -0.5, upper = 0.5) # favors values > 50%
  muprior_b_many  <- my_rnorm(2, mu = 0.3 , sigma = 0.2, lower = -0.5, upper = 0.5) # favors values > 50%
  muprior_b_few   <- my_rnorm(2, mu = -0.3, sigma = 0.2, lower = -0.5, upper = 0.5) # favors values < 50%
  
  muprior_a <- matrix(0, nrow = predat$D, ncol = t)
  muprior_g <- matrix(0, nrow = predat$D, ncol = t)     
  for(d in 1:predat$D){
    
    muprior_a[d,] <- my_rnorm(2, mu = -4, sigma = 1, lower = -6, upper = 0)
    muprior_g[d,] <- my_rnorm(2, mu  = 0, sigma = 0.2, lower = 0, upper = 0.5)
    
  }
   
  # group-level standard deviation:
    sdprior_a <- 1/rgamma(predat$D, 2, 1) # more variability since log-transformed
    sdprior_b <- 1/rgamma(predat$D, 5, 1)
    sdprior_g <- 1/rgamma(predat$D, 5, 1)
    
  # group-level correlation:
    # between two timepoints (we found moderate correlations)
    # I assume we want to estimate for beta and gamma and each quantifier the correlation separately
    rho_a_raw <- NULL
    rho_b_raw <- NULL
    rho_g_raw <- NULL
    for(d in 1:predat$D){
      
      rho_a_raw[d] <- rbeta(1, 4, 4) # favor values around 0
      rho_b_raw[d] <- rbeta(1, 5, 3) # favor values between 0 and 0.5
      rho_g_raw[d] <- rbeta(1, 5, 3) # favor values between 0 and 0.5
      
    }
    
  # generated quantities:
  rho_a <- (rho_a_raw * 2) - 1 # vagueness: not enough variability to detect correlations
  rho_b <- (rho_b_raw * 2) - 1
  rho_g <- (rho_g_raw * 2) - 1
  
  # threshold: timepoint 1 and 2
  beta_t1_more_prior  <- my_rmvnorm(nsamp=predat$I, Mu=muprior_b_more , Sigma=makeS(rho_b_raw[1], sdprior_b), lower = -0.5, upper = 0.5)[,1]
  beta_t1_fewer_prior <- my_rmvnorm(nsamp=predat$I, Mu=muprior_b_fewer, Sigma=makeS(rho_b_raw[2], sdprior_b), lower = -0.5, upper = 0.5)[,1]  
  beta_t1_most_prior  <- my_rmvnorm(nsamp=predat$I, Mu=muprior_b_most , Sigma=makeS(rho_b_raw[3], sdprior_b), lower = -0.5, upper = 0.5)[,1]  
  beta_t1_many_prior  <- my_rmvnorm(nsamp=predat$I, Mu=muprior_b_many , Sigma=makeS(rho_b_raw[4], sdprior_b), lower = -0.5, upper = 0.5)[,1]  
  beta_t1_few_prior   <- my_rmvnorm(nsamp=predat$I, Mu=muprior_b_few  , Sigma=makeS(rho_b_raw[5], sdprior_b), lower = -0.5, upper = 0.5)[,1]  
  beta_t2_more_prior  <- my_rmvnorm(nsamp=predat$I, Mu=muprior_b_more , Sigma=makeS(rho_b_raw[1], sdprior_b), lower = -0.5, upper = 0.5)[,2]
  beta_t2_fewer_prior <- my_rmvnorm(nsamp=predat$I, Mu=muprior_b_fewer, Sigma=makeS(rho_b_raw[2], sdprior_b), lower = -0.5, upper = 0.5)[,2]  
  beta_t2_most_prior  <- my_rmvnorm(nsamp=predat$I, Mu=muprior_b_most , Sigma=makeS(rho_b_raw[3], sdprior_b), lower = -0.5, upper = 0.5)[,2]  
  beta_t2_many_prior  <- my_rmvnorm(nsamp=predat$I, Mu=muprior_b_many , Sigma=makeS(rho_b_raw[4], sdprior_b), lower = -0.5, upper = 0.5)[,2]  
  beta_t2_few_prior   <- my_rmvnorm(nsamp=predat$I, Mu=muprior_b_few  , Sigma=makeS(rho_b_raw[5], sdprior_b), lower = -0.5, upper = 0.5)[,2]
  
  # vagueness and response noise: timepoint 1 and 2
  alpha_t1_raw_prior <- matrix(0, nrow = predat$I, ncol = predat$D)
  alpha_t2_raw_prior <- matrix(0, nrow = predat$I, ncol = predat$D)
  alpha_t1_prior     <- matrix(0, nrow = predat$I, ncol = predat$D)
  alpha_t2_prior     <- matrix(0, nrow = predat$I, ncol = predat$D)
  gamma_t1_prior     <- matrix(0, nrow = predat$I, ncol = predat$D)
  gamma_t2_prior     <- matrix(0, nrow = predat$I, ncol = predat$D)
  for(d in 1:predat$D){
    
    # vagueness: timepoint 1 and 2
    alpha_t1_raw_prior[, d] <- rmvnorm(predat$I, muprior_a[d, ], makeS(rho_a[d], sdprior_a[d]))[,1]
    alpha_t2_raw_prior[, d] <- rmvnorm(predat$I, muprior_a[d, ], makeS(rho_a[d], sdprior_a[d]))[,2]
    alpha_t1_prior[, d]     <- exp(alpha_t1_raw_prior[, d])
    alpha_t2_prior[, d]     <- exp(alpha_t2_raw_prior[, d])
    
    # response error: timepoint 1 and 2
    gamma_t1_prior[, d] <- my_rmvnorm(nsamp = predat$I,
                                      Mu    = muprior_g[d, ],
                                      Sigma = makeS(rho_g[d], sdprior_g[d]),
                                      lower = 0,
                                      upper = 0.5)[,1]
    gamma_t2_prior[, d] <- my_rmvnorm(nsamp = predat$I,
                                      Mu    = muprior_g[d, ],
                                      Sigma = makeS(rho_g[d], sdprior_g[d]),
                                      lower = 0,
                                      upper = 0.5)[,2]
    
  }
  
  # timepoint 1
  for (n in 1:predat$N_t1) {
    
    mu_pred_t1[n, s] <- predat$more_t1[n] * (predat$cperc_t1[n] - beta_t1_more_prior[sub_t1[n]]) / alpha_t1_prior[sub_t1[n], 1] +
      predat$fewer_t1[n]  * (predat$cperc_t1[n] - beta_t1_fewer_prior[sub_t1[n]]) / alpha_t1_prior[sub_t1[n], 2] +
      predat$most_t1[n]  * (predat$cperc_t1[n] - beta_t1_most_prior[sub_t1[n]]) / alpha_t1_prior[sub_t1[n], 3] +
      predat$many_t1[n]  * (predat$cperc_t1[n] - beta_t1_many_prior[sub_t1[n]]) / alpha_t1_prior[sub_t1[n], 4] +
      predat$few_t1[n]  * (predat$cperc_t1[n] - beta_t1_few_prior[sub_t1[n]]) / alpha_t1_prior[sub_t1[n], 5] 
    
    p_pred_t1[n, s] <- predat$more_t1[n] * gamma_t1_prior[sub_t1[n], 1] +
      predat$fewer_t1[n] * gamma_t1_prior[sub_t1[n], 2] +
      predat$most_t1[n] * gamma_t1_prior[sub_t1[n], 3] +
      predat$many_t1[n] * gamma_t1_prior[sub_t1[n], 4] +
      predat$few_t1[n] * gamma_t1_prior[sub_t1[n], 5] +
      (1 - 2 * (predat$more_t1[n] * gamma_t1_prior[sub_t1[n], 1] +
                  predat$fewer_t1[n] * gamma_t1_prior[sub_t1[n], 2] +
                  predat$most_t1[n] * gamma_t1_prior[sub_t1[n], 3] +
                  predat$many_t1[n] * gamma_t1_prior[sub_t1[n], 4] +
                  predat$few_t1[n] * gamma_t1_prior[sub_t1[n], 5])) *
    (1 - exp(-pmax(mu_pred_t1[n, s], .001)))
    
    y_pred_t1[n, s] <- rbinom(1, 1, p_pred_t1[n, s])
    
  }
  
  # timepoint 2
  for (n in 1:predat$N_t2) {
    
    mu_pred_t2[n, s] <- predat$more_t2[n] * (predat$cperc_t2[n] - beta_t2_more_prior[sub_t2[n]]) / alpha_t2_prior[sub_t2[n], 1] +
      predat$fewer_t2[n]  * (predat$cperc_t2[n] - beta_t2_fewer_prior[sub_t2[n]]) / alpha_t2_prior[sub_t2[n], 2] +
      predat$most_t2[n]  * (predat$cperc_t2[n] - beta_t2_most_prior[sub_t2[n]]) / alpha_t2_prior[sub_t2[n], 3] +
      predat$many_t2[n]  * (predat$cperc_t2[n] - beta_t2_many_prior[sub_t2[n]]) / alpha_t2_prior[sub_t2[n], 4] +
      predat$few_t2[n]  * (predat$cperc_t2[n] - beta_t2_few_prior[sub_t2[n]]) / alpha_t2_prior[sub_t2[n], 5] 
    
    p_pred_t2[n, s] <- predat$more_t2[n] * gamma_t2_prior[sub_t2[n], 1] +
      predat$fewer_t2[n] * gamma_t2_prior[sub_t2[n], 2] +
      predat$most_t2[n] * gamma_t2_prior[sub_t2[n], 3] +
      predat$many_t2[n] * gamma_t2_prior[sub_t2[n], 4] +
      predat$few_t2[n] * gamma_t2_prior[sub_t2[n], 5] +
      (1 - 2 * (predat$more_t2[n] * gamma_t2_prior[sub_t2[n], 1] +
                  predat$fewer_t2[n] * gamma_t2_prior[sub_t2[n], 2] +
                  predat$most_t2[n] * gamma_t2_prior[sub_t2[n], 3] +
                  predat$many_t2[n] * gamma_t2_prior[sub_t2[n], 4] +
                  predat$few_t2[n] * gamma_t2_prior[sub_t2[n], 5])) *
    (1 - exp(-max(mu_pred_t1[n, s], 0)))
    
    y_pred_t2[n, s] <- rbinom(1, 1, p_pred_t2[n, s])
    
  }

}

# vec <- c(0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100)
# # analyze response patterns: t1
# predat$qq     <- as.numeric(dat_t1$qq)
# predat$cperc_t1       <- predat$cperc_t1 * 100 + 50
# predat$percent_cat_t1 <- cut(predat$cperc_t1,
#                              breaks=vec,
#                              labels=as.character((vec + 5)[-length(vec)]))
# # analyze response patterns: t2
# predat$cperc_t2       <- predat$cperc_t2 * 100 + 50
# predat$percent_cat_t2 <- cut(predat$cperc_t2,
#                              breaks=vec,
#                              labels=as.character((vec + 5)[-length(vec)]))
# # need to fix predat
# analyzePredictedResponsesR(y_pred_t1, predat, testtime = 't1')
# analyzePredictedResponsesR(y_pred_t2, predat, testtime = 't2')
# plot_list_t1 <- analyzePredictedResponsesR(y_pred_t1, predat, individual = TRUE, testtime = 't1')
# plot_list_t2  <-analyzePredictedResponsesR(y_pred_t2, predat, individual = TRUE, testtime = 't2')
# correlations between t1 and t2: alpha, beta, and gamma parameters?

  # plot(rowMeans(p_pred_t1), rowMeans(p_pred_t2))
  # cor(rowMeans(p_pred_t1), rowMeans(p_pred_t2))

```

Conclusion:
- gamma needs to be restricted to values smaller than 0.5 (as it flips the distribution otherwise)
- beta needs to range between -0.5 and 0.5 to be on the same scale as cperc
- alpha: draw from normal distribution, then exponentiate, priors can be reused
- Watch out for Opaque priors when we fit this model in stan!

```{r initialize_b_thresholds, eval = TRUE}
# 1 = FewerThanHalf
# 2 = MoreThanHalf
# 3 = Few
# 4 = Most
# 5 = Many
mu_b_mean      <- c(0, 0, -0.2, 0.2, 0.2)
quant          <- c('Fewer', 'More', 'Few', 'Most', 'Many')
```

```{r try_out_model, eval = FALSE}
# try out model for quantifier "fewer than half"
# this code is to assess model convergence (divergent transitions, Rhat, chains, correlations)
predat       <- getDat(dat_t1, dat_t2, prior = prior)
# predat       <- getDat(dat_t1, prior = prior) # univariate case
predat$mu_b_mean <- mu_b_mean[1]

logmodfit <- myRunner(predat
                      , addparams = addparams
                      , mod = model_list[[1]]
                      , iter = niter
                      , ncores = nchains
                      , warmup = nwarmup
                      , control = list(adapt_delta = .99, max_treedepth = 17)
                      , nchains = nchains)

print(names(logmodfit))
# load(paste0('../output/predat_sim_', filename))

# parameter distributions
pairs(logmodfit, pars = c("sigma_mu_log_a"))
pairs(logmodfit, pars = c("sigma_mu_b"))
pairs(logmodfit, pars = c("sigma_mu_g"))
pairs(logmodfit, pars = c("mu_log_a"))
pairs(logmodfit, pars = c("mu_b", "rho_raw_b"))
pairs(logmodfit, pars = c("mu_g"))

# number of divergent transitions
colnames(rstan::get_num_divergent(logmodfit)) # no divergent transitions

# Rhats
list_of_draws <- rstan::extract(logmodfit)
Rhat          <- summary(logmodfit)$summary[,'Rhat']
Rhat          <- Rhat[!(names(Rhat) %in% c('L_Sigma_b[1,2]', 'L_Sigma_g[1,2]'))]
all((Rhat > 0.95 & Rhat < 1.05))

# traceplots
traceplot(logmodfit, pars = c("sigma_mu_log_a"))
traceplot(logmodfit, pars = c("sigma_mu_b"))
traceplot(logmodfit, pars = c("mu_b"))
traceplot(logmodfit, pars = c("rho_raw_b"))
traceplot(logmodfit, pars = c("b[1,1]", "b[1,2]"))

# look at samples
samples <- removeDivergentTransitions(logmodfit, nchains = nchains)
var_names <- colnames(samples)[!grepl('pred', colnames(samples)) & 
                                            !grepl('lp__', colnames(samples))]
round(apply(samples[, var_names], 2, median), 3) 

# check for opaque priors in the group-level parameters
hist(samples[, 'mu_g[1]'], freq = FALSE)
lines(density(rnorm(1e4, 0.17, 0.05))) # it's still slightly off: rnorm(0.19, 0.05)
hist(samples[, 'sigma_mu_g[1]'], freq = FALSE)
lines(density(1/rgamma(1e4, 12, 1))) # it's still slightly off: 1/rgamma(1e4, 13, 1)
hist(samples[, 'rho_raw_g'], freq = FALSE)
lines(density(rbeta(1e4, 4, 4)))
hist(samples[, 'g[1,2]'])

hist(samples[, 'mu_b[1]'], freq = FALSE)
lines(density(rnorm(1e4, 0, 0.1)))
hist(samples[, 'sigma_mu_b[1]'], freq = FALSE)
lines(density(1/rgamma(1e4, 6, 1)))
hist(samples[, 'rho_raw_b'], freq = FALSE)
lines(density(rbeta(1e4, 4, 4)))

hist(samples[, 'mu_log_a[1]'], freq = FALSE)
lines(density(rnorm(1e4, -3, 1)))
hist(samples[, 'sigma_mu_log_a[1]'], freq = FALSE)
lines(density(1/rgamma(1e4, 2, 1)))

# for the univariate case
pairs(logmodfit, pars = c("sigma_mu_log_a", "sigma_mu_b"))
pairs(logmodfit, pars = c("sigma_mu_b", "sigma_mu_g"))
pairs(logmodfit, pars = c("mu_b", "mu_g", "mu_log_a"))

hist(samples[, 'mu_g'], freq = FALSE)
lines(density(rnorm(1e4, 0.17, 0.05))) # it's still slightly off: rnorm(0.19, 0.05)
hist(samples[, 'sigma_mu_g'], freq = FALSE)
lines(density(1/rgamma(1e4, 12, 1))) # it's still slightly off: 1/rgamma(1e4, 13, 1)
hist(samples[, 'g[1]'])

hist(samples[, 'mu_b'], freq = FALSE)
lines(density(rnorm(1e4, 0, 0.1)))
hist(samples[, 'sigma_mu_b'], freq = FALSE)
lines(density(1/rgamma(1e4, 6, 1)))

hist(samples[, 'mu_log_a'], freq = FALSE)
lines(density(rnorm(1e4, -3, 1)))
hist(samples[, 'sigma_mu_log_a'], freq = FALSE)
lines(density(1/rgamma(1e4, 2, 1)))
```


```{r run-model, eval = FALSE}
set.seed(4491)
for(d in 1:length(quant)){
  
  predat           <- getDat(dat_t1, dat_t2, prior = prior)
  predat$mu_b_mean <- mu_b_mean[d]
  
  logmodfit <- myRunner(predat
                      , addparams = addparams
                      , mod = model_list[[1]]
                      , iter = niter
                      , ncores = nchains
                      , warmup = nwarmup
                      , control = list(adapt_delta = .99, max_treedepth = 17)
                      , nchains = nchains)
  save(logmodfit, file = paste0('../output/',quant[d], '_', gsub('stan', 'rda', filename)))

# if it is a prior distribution
if(prior){
  
  # (1) remove divergent transitions
  samples_processed <- removeDivergentTransitions(logmodfit, nchains = nchains)
  
  # (2) remove __lp column from samples
  var_names <- colnames(samples_processed)[!grepl('pred', colnames(samples_processed)) & 
                                            !grepl('lp__', colnames(samples_processed))]
  samples_pars <- samples_processed[, var_names]
  
  # (3) compute the posterior means and variances
  mu_prior  <- apply(samples_pars, 2, mean) 
  var_prior <- apply(samples_pars, 2, var) 
  
  # (4) thin the chains and export parameter values from priors
  samples_pars <- samples_pars[seq(from=1, to=nrow(samples_pars), by = 50),]
  write.csv2(samples_pars,
             paste0('../output/prior_pred_all_', quant[d], '_', modelname, '.csv'),
             row.names = FALSE)
  
  # (5) thin the chains of predicted values
  pred_names     <- colnames(samples_processed)[grepl('y_pred', colnames(samples_processed))]
  pred_responses <- samples_processed[, pred_names]
  pred_responses <- pred_responses[seq(from=1, to=nrow(pred_responses), by = 50),]
  
  # (6) export predicted responses from priors
  colnames(pred_responses) <- paste0('y_pred_', 1:length(pred_names))
  write.csv2(pred_responses,
             paste0('../output/prior_pred_', quant[d], '_', modelname, '.csv'),
             row.names = FALSE)
  
  # (7) export mean and variance from prior
  write.csv2(mu_prior, paste0('../output/sim_mu_prior_', modelname, '_', quant[d], '.csv'), 
             row.names = FALSE)
  write.csv2(var_prior, paste0('../output/sim_var_prior_', modelname, '_', quant[d], '.csv'), 
             row.names = FALSE)
  }
}
```

```{r run-model-sonia-prior, eval = FALSE}
# make sure you loaded the correct dataset: "/exp1-preprocessed-all.csv"
# make sure you load the correct model: "mod_partial_exp_t1_prior"
filename   <- "mod_partial_exp_t1_prior.stan"
model_list <- rstan::stan_model(file = paste0('../models/', filename))

# fit the model to each quantifier individually
dat_t1    <- read.csv('../data/exp1-preprocessed-all.csv', header = TRUE)
# make quantifiers identifiable: keep only first word so it matches quant
dat_t1$quant <- stringr::word(dat_t1$quant, 1)

set.seed(4491)
for(d in 1:length(quant)){
  
  dat              <- dat_t1[grepl(paste0(quant[d], '$'), dat_t1$quant), ] 
  predat           <- getDat(dat, prior = FALSE)
  predat$mu_b_mean <- mu_b_mean[d]
  
  logmodfit <- myRunner(predat
                      , mod = model_list
                      , iter = niter
                      , ncores = nchains
                      , warmup = nwarmup
                      , control = list(adapt_delta = .99, max_treedepth = 17)
                      , nchains = nchains)

  # (1) remove divergent transitions and thin the chains
  samples_processed <- removeDivergentTransitions(logmodfit, nchains = nchains)
  samples_processed <- samples_processed[seq(from=1, to=nrow(samples_processed), by = 50),]
  
  # (2) compute the posterior means
  mus  <- apply(samples_processed, 2, mean)
  
  # (2) export parameter values
  write.csv2(t(as.data.frame(mus)), paste0('../output/ramotowska_t1_all_mu_prior', quant[d], '.csv'), 
             row.names = FALSE)
  write.csv2(samples_processed,
             paste0('../output/ramotowska_t1_all_prior', quant[d], '.csv'),
             row.names = FALSE)
}
```

```{r run-model-sonia, eval = FALSE}
# make sure you loaded the correct dataset: "/exp1-preprocessed-all.csv"
# make sure you load the correct model: "mod_partial_exp_t1"
filename   <- "mod_partial_exp_t1.stan"
model_list <- rstan::stan_model(file = paste0('../models/', filename))

# fit the model to each quantifier individually
dat_t1    <- read.csv('../data/exp1-preprocessed-all.csv', header = TRUE)
# make quantifiers identifiable: keep only first word so it matches quant
dat_t1$quant <- stringr::word(dat_t1$quant, 1)

set.seed(4491)
for(d in 2:length(quant)){
  
  dat              <- dat_t1[grepl(paste0(quant[d], '$'), dat_t1$quant), ] 
  predat           <- getDat(dat, prior = FALSE)
  predat$mu_b_mean <- mu_b_mean[d]
  
  logmodfit <- myRunner(predat
                      , mod = model_list
                      , iter = niter
                      , ncores = nchains
                      , warmup = nwarmup
                      , control = list(adapt_delta = .99, max_treedepth = 17)
                      , nchains = nchains)

  # (1) remove divergent transitions and thin the chains
  samples_processed <- removeDivergentTransitions(logmodfit, nchains = nchains)
  samples_processed <- samples_processed[seq(from=1, to=nrow(samples_processed), by = 50),]
  
  # (2) compute the posterior means
  mus  <- apply(samples_processed, 2, mean)
  
  # (2) export parameter values
  write.csv2(t(as.data.frame(mus)), paste0('../output/ramotowska_t1_all_mu_', quant[d], '.csv'), 
             row.names = FALSE)
  write.csv2(samples_processed,
             paste0('../output/ramotowska_t1_all_', quant[d], '.csv'),
             row.names = FALSE)
}
```

```{r visualize-posterior-predictive-group, eval = FALSE}
par(mfrow=c(2, 3))

for(d in 1:5){
  mu <-  read.csv2(paste0('../output/ramotowska_t1_all_mu_', quant[d], '.csv'),
                         header = TRUE, sep =';')
  dat                 <- dat_t1[grepl(paste0(quant[d], '$'), dat_t1$quant), ] 
  prop_observed_group <- tapply(dat$resp, dat$percent_cat, mean, na.rm = T)
  cperc_observed <- c(25 - ((25-1) / 2),
                      40 - ((40-26) / 2), 
                      50 - ((50-41) / 2), 
                      60 - ((60-51) / 2), 
                      75 - ((75-61) / 2), 
                      100 - ((100-76) / 2))/100 - 0.5
  cperc <- seq(-0.5, 0.5, length.out = 1e3)
  
  # plot
  plot(cperc, modelPredictions(alpha = exp(mu[colnames(mu) == 'mu_log_a']), 
                               beta  = mu[colnames(mu) == 'mu_b'], 
                               gamma = mu[colnames(mu) == 'mu_g'], 
                               cperc = cperc),
       type = 'l',
       lwd = 2,
       las = 1,
       bty = 'n',
       main = quant[d],
       xlab = 'Presented Proportion',
       ylab = 'p("True")',
       xlim = c(-0.5, 0.5),
       ylim = c(0, 1))
points(cperc_observed, prop_observed_group, pch = 2, lwd = 2)
}
```

```{r prior-posterior-group, eval = FALSE}
par(mfrow=c(2, 3))

for(d in 1:5){
  prior <- read.csv2(paste0('../output/ramotowska_t1_all_prior', quant[d], '.csv'),
                         header = TRUE, sep =';')
  post  <-  read.csv2(paste0('../output/ramotowska_t1_all_', quant[d], '.csv'),
                         header = TRUE, sep =';')
  # mus
  plot(density(exp(post[,colnames(post) == 'mu_log_a'])),
       xlim = c(0, 0.4),
       las = 1,
       bty = 'n',
       main = paste('Mu Vagueness:', quant[d]))
  lines(density(exp(prior[,colnames(prior) == 'mu_log_a'])), lty = 3)
  
  plot(density(post[,colnames(post) == 'mu_b']),
       xlim = c(-0.5, 0.5),
       las = 1,
       bty = 'n',
       main = paste('Mu Threshold:', quant[d]))
  lines(density(prior[,colnames(prior) == 'mu_b']), lty = 3)
  
  plot(density(post[,colnames(post) == 'mu_g']),
       xlim = c(0, 0.5),
       las = 1,
       bty = 'n',
       main = paste('Mu Response noise:', quant[d]))
  lines(density(prior[,colnames(prior) == 'mu_g']), lty = 3)
  
  # sigmas
  plot(density(exp(post[,colnames(post) == 'sigma_mu_log_a'])),
       xlim = c(0, 4),
       las = 1,
       bty = 'n',
       main = paste('Sd Vagueness:', quant[d]))
  lines(density(exp(prior[,colnames(prior) == 'sigma_mu_log_a'])), lty = 3)
  
  plot(density(post[,colnames(post) == 'sigma_mu_b']),
       xlim = c(0, 0.5),
       las = 1,
       bty = 'n',
       main = paste('Sd Threshold:', quant[d]))
  lines(density(prior[,colnames(prior) == 'sigma_mu_b']), lty = 3)
                      
  plot(density(post[,colnames(post) == 'sigma_mu_g']),
       xlim = c(0, 0.5),
       las = 1,
       bty = 'n',
       main = paste('Sd Response noise:', quant[d]))
  lines(density(prior[,colnames(prior) == 'sigma_mu_g']), lty = 3)
  
}
```

```{r visualize-posterior-predictive-ind, eval = FALSE}
par(mfrow=c(2, 3))


for(d in 1:3){
  mu <-  read.csv2(paste0('../output/ramotowska_t1_all_mu_', quant[d], '.csv'),
                         header = TRUE, sep =';')
  nind                <- length(colnames(mu)[grepl('^log_a', colnames(mu))])
  dat                 <- dat_t1[grepl(paste0(quant[d], '$'), dat_t1$quant), ] 
  prop_observed_ind   <- tapply(dat$resp, list(dat$percent_cat, dat$workerid), mean, na.rm = T)
  cperc_observed <- c(25 - ((25-1) / 2),
                      40 - ((40-26) / 2), 
                      50 - ((50-41) / 2), 
                      60 - ((60-51) / 2), 
                      75 - ((75-61) / 2), 
                      100 - ((100-76) / 2))/100 - 0.5
  cperc <- seq(-0.5, 0.5, length.out = 1e3)
  
  for(i in 1:nind){
    plot(cperc, modelPredictions(alpha = exp(mu[colnames(mu) == paste0('log_a.', i, '.')]), 
                                 beta  = mu[colnames(mu) == paste0('b.', i, '.')], 
                                 gamma = mu[colnames(mu) == paste0('g.', i, '.')], 
                                 cperc = cperc),
         type = 'l',
         las = 1,
         lwd = 2,
         bty = 'n',
         main = paste(quant[d], i),
         xlab = 'Presented Proportion',
         ylab = 'p("True")',
         xlim = c(-0.5, 0.5),
         ylim = c(0, 1))
    points(cperc_observed, prop_observed_ind[,i], pch = 2, col = 'red', lwd = 2)
  }
}
```

# Model validation 

```{r set-inits-val, eval = TRUE}
d = 3
prior      <- FALSE
modelname  <- 'mod_partial_exp'
filename   <- paste0(quant[d], '_', gsub('stan', 'rda', filename))
```

## Plot model predictions

```{r, eval = FALSE}
sbc_prior <- read.csv2(paste0('../output/prior_pred_all_', quant[d], '_', modelname, '.csv'),
                       header = TRUE, sep =';')[1,] # only needed for colnames xD
mu_prior   <- t(read.csv2(paste0('../output/sim_mu_prior_', modelname, '_', quant[d],'.csv'), 
                          header = TRUE, sep =';'))
colnames(mu_prior) <- colnames(sbc_prior)
cperc <- seq(-0.5, 0.5, length.out = 1e3)

# mu_g: take expected value and median of truncated normal distribution
# see two-sided truncation at: https://en.wikipedia.org/wiki/Truncated_normal_distribution
plot(cperc, modelPredictions(alpha = exp(mu_prior[colnames(mu_prior) == 'mu_log_a.1.']), 
                             beta = mu_prior[colnames(mu_prior) == 'mu_b.1.'], 
                             gamma = mu_prior[colnames(mu_prior) == 'mu_g.1.'], 
                             cperc),
     type = 'l',
     lwd = 2,
     las = 1,
     bty = 'n',
     xlab = 'Presented Proportion',
     ylab = 'p("True")',
     xlim = c(-0.5, 0.5),
     ylim = c(0, 1))
lines(cperc, modelPredictions(alpha = exp(mu_prior[colnames(mu_prior) == 'mu_log_a.2.']), 
                             beta = mu_prior[colnames(mu_prior) == 'mu_b.2.'], 
                             gamma = abs(mu_prior[colnames(mu_prior) == 'mu_g.2.']), 
                             cperc),
      lty = 3,
      lwd = 2)
```


## Inspect predicted response patterns

```{r inspect-predicted-responses, eval = FALSE}
predat    <- getDat(dat_t1, dat_t2, prior = prior)
vec       <- c(0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100)
predat$qq <- as.numeric(dat_t1$qq)
predat$cperc_t1       <- predat$cperc_t1 * 100 + 50
predat$percent_cat_t1 <- cut(predat$cperc_t1,
                             breaks=vec,
                             labels=as.character((vec + 5)[-length(vec)]))
predat$cperc_t2       <- predat$cperc_t2 * 100 + 50
predat$percent_cat_t2 <- cut(predat$cperc_t2,
                             breaks=vec,
                             labels=as.character((vec + 5)[-length(vec)]))

y_pred_dat <- list()

for(d in 1:length(quant)){
  y_pred_dat[[d]] <- t(read.csv(paste0('../output/prior_pred_', quant[d], '_', modelname, '.csv'),
                       header = TRUE, sep = ';'))
}

# predicted responses on a group level
for(d in 1:length(quant)){
  predat$quant_name <- quant[d]
  analyzePredictedResponsesR(y_pred=y_pred_dat[[d]][1:4000,]   , predat, testtime = 't1', individual = TRUE)
  analyzePredictedResponsesR(y_pred=y_pred_dat[[d]][4001:8000,], predat, testtime = 't2', individual = TRUE)
}
```

## Inspect parameter distribution and convergence

```{r load-samples, eval = FALSE}
source('../helper_functions/load_samples.R')
```

```{r check-model-convergence, eval = FALSE}
parameters      <- c('a', 'b', 'g')
hyperparameters <- c('sigma_mu_log_a', 'sigma_mu_b', 'sigma_mu_g', 
                     'mu_log_a', 'mu_b', 'mu_g',
                     "rho_raw_b", "rho_raw_a", "rho_raw_g")

convergence_diagnostics <- checkModelConvergence(list_of_draws, parameters, hyperparameters)
convergence_diagnostics$reached_convergence
convergence_diagnostics$Rhat_params
for(i in 1:nchains) convergence_diagnostics$correlation_plot[[i]]
```

```{r remove-divergent-transitions, eval = FALSE}
# number of divergent transitions
colnames(rstan::get_num_divergent(logmodfit))
# remove divergent transitions before any further analysis
samples_processed <- removeDivergentTransitions(logmodfit, nchains = nchains)
```

```{r check-parameter-distributions, eval = FALSE}
# on the individual level
pars <- matrix(c('log_a[1,1]', 'log_a[1,2]',
                 'a[1,1]', 'a[1,2]', 
                 'b[1,1]', 'b[1,2]', 
                 'g[1,1]', 'g[1,2]'), ncol = 2, byrow = TRUE)
for(i in 1:nrow(pars)) pairs(logmodfit, pars = pars[i,])
```

## Prior and posterior predictive checks

- Posterior predictive: match between observed and estimated response patterns
- Prior predictive
  * monotonic increase in the probability to respond 'true'
  * small response error: 0-20 and 80 - 100, 80% should be in accordance for the expectations
  * vagueness: at threshold parameter: maximum uncertainty, about 50% probability to say 'true'
- To-Do: individual response patterns generated from the prior predictives make no sense

```{r inspect-response-patterns, eval = FALSE}
if(max(predat$cperc) < 1) predat$cperc <- predat$cperc * 100 + 50
predat$percent_cat <- cut(predat$cperc,
                      breaks=c(0, 20, 40, 60, 80, 100),
                      labels=c("1-20","21-40","41-60", "61-80", "81-100"))
predat$qq    <- ifelse(predat$fewer == 1, 1, 2)

predResponses <- analyzePredictedResponses(samples_processed, predat,
                                           prior, modelname, hyperparameters)
# prior predict: random row
# posterior predict: aggregated over all
if(prior){
  
  predResponses$monotonic_increase
  predResponses$small_response_error
  predResponses$constrained_vagueness
      
}

par(mfrow=c(2,2))
for(i in 1:length(predResponses$pred_plot)){
 replayPlot(predResponses$pred_plot[[i]])
}
```

# Simulation study

### Sampling characteristics

Convergence diagnostics, sampling properties

## Computational faithfulness

Simulation based calibration.
- step 1: simulate data from the prior distribution
- step 2: fit the model to the simulated data

```{r initialize-simulation-study}
d = 4 # choose quantifier
model_list     <- rstan::stan_model(file = paste0('../models/', modelname, '.stan'))
nsims_target   <- 100
nsims_ready    <- length(list.files('../output/', 
                                    pattern=paste0("sim_mu_post_", modelname, '_', quant[d]), 
                                    full.names = TRUE, recursive = TRUE))
nsims          <- nsims_target - nsims_ready

pred_responses <- read.csv2(paste0('../output/prior_pred_', quant[d], '_', modelname, '.csv'),
                             header = TRUE, sep =';')
mu_prior   <- t(read.csv2(paste0('../output/sim_mu_prior_', modelname, '_', quant[d],'.csv'), 
                          header = TRUE, sep =';'))
var_prior  <- t(read.csv2(paste0('../output/sim_var_prior_', modelname, '_', quant[d],'.csv'), 
                          header = TRUE, sep =';'))
# load prior parameters samples (divergent transitions already removed)
sbc_prior <- read.csv2(paste0('../output/prior_pred_all_', quant[d], '_', modelname, '.csv'),
                       header = TRUE, sep =';')[1:nsims_target,] 
par_names_post <- par_names_prior <- colnames(sbc_prior)
colnames(mu_prior) <- colnames(var_prior) <- colnames(sbc_prior)
```


```{r run-simulation-study, eval = FALSE}
niter   <- 1500
nwarmup <- 500

for(i in 1:nsims){
  
  predat         <- getDat(dat_t1, dat_t2, prior = FALSE)
  predat$y_t1    <- as.integer(pred_responses[nsims_ready + i, 1:predat$N_t1])
  predat$y_t2    <- as.integer(pred_responses[nsims_ready + i, 
                                              (predat$N_t1 + 1):(predat$N_t2 + predat$N_t1)])
  predat$mu_b_mean  <- mu_b_mean[d]
  
  logmodfit <- myRunner(predat
                      , addparams = addparams
                      , mod = model_list
                      , iter = niter
                      , ncores = nchains
                      , warmup = nwarmup
                      , control = list(adapt_delta = .97, max_treedepth = 14)
                      , nchains = nchains)

  # remove divergent transitions
  sbc_post <- removeDivergentTransitions(logmodfit, nchains = nchains)
  sbc_post <- sbc_post[, -which(colnames(sbc_post) == 'lp__')]
  par_names_post  <- colnames(sbc_post) 
  par_names_prior <- colnames(sbc_prior)
  # compute the posterior medians and variances
  mu_post  <- apply(sbc_post, 2, mean)
  var_post <- apply(sbc_post, 2, var) 
  # compute the 50% and 80% credible intervals
  ci_post  <- apply(sbc_post, 2, function(x) quantile(x, c(0.1, 0.9, 0.25, 0.75)))
  # thin the chains
  sbc_post <- sbc_post[seq(from=1, to=nrow(sbc_post), by = 20),]
  
  rank <- data.frame(matrix(nrow = 1, ncol = ncol(sbc_post)))
  colnames(rank) <- par_names_post
  
  # loop over parameters
  for (j in 1:length(par_names_post)) {
    
    # count the posterior samples that are smaller than the draw from the prior
    rank[, j] <- sum(sbc_post[, par_names_post[j]] < sbc_prior[nsims_ready + i, par_names_prior[j]])
    
  }
  
  # export results
  write.csv2(mu_post, paste0('../output/sim_mu_post_', modelname, '_', quant[d],nsims_ready + i, '.csv'), row.names = FALSE)
  write.csv2(var_post, paste0('../output/sim_var_post_', modelname, '_', quant[d],nsims_ready + i, '.csv'), row.names = FALSE)
  write.csv2(ci_post, paste0('../output/sim_ci_post_', modelname, '_', quant[d],nsims_ready + i, '.csv'), row.names = FALSE)
  write.csv2(rank, paste0('../output/sim_rank_', modelname, '_', quant[d],nsims_ready + i, '.csv'), row.names = FALSE)
  
}

```


```{r load-sim-files}
npars          <- ncol(sbc_prior)

# compute necessary quantities
  rank           <- data.frame(matrix(nrow = 0, ncol = npars))
  mu_post        <- data.frame(matrix(nrow = 0, ncol = npars))
  var_post       <- data.frame(matrix(nrow = 0, ncol = npars))
  ci_post        <- list()
  colnames(rank) <- colnames(mu_post) <- colnames(var_post) <- par_names_post

d=2
# for(d in 1:5){
  for(i in 1:nsims_ready){
    
     mu_post[nrow(mu_post) + 1,] <-  t(read.csv2(paste0('../output/sim_mu_post_', modelname, '_', quant[d], i, '.csv'),
                             header = TRUE, sep =';'))
     var_post[nrow(var_post) + 1,] <-  t(read.csv2(paste0('../output/sim_var_post_', modelname, '_', quant[d], i, '.csv'),
                             header = TRUE, sep =';'))
     rank[nrow(rank) + 1,] <-  t(read.csv2(paste0('../output/sim_rank_', modelname, '_', quant[d], i, '.csv'),
                             header = TRUE, sep =';'))
      ci_post[[length(ci_post) + 1]] <-  read.csv2(paste0('../output/sim_ci_post_', modelname, '_', quant[d], i, '.csv'),
                             header = TRUE, sep =';')
  }
# }

```


- step 3: compute the rank statistics of the prior estimates with the 'posterior' estimates
- we are interested in the hyper-level parameters
- each posterior simulation should return only one value, that is, the number of samples smaller than the prior sample

```{r sbc}
# plot the historgrams
par(mfrow=c(2, 3))

for(j in 1:npars){
    
  hist(rank[, j], las = 1, 
       main = par_names_post[j], 
       xlab = 'Rank')
    
}
```


## Model sensitivity

- goal 1: how well does the posterior mean match the true data generating process
  * sample from the prior of the model (done previously)
  * simulate data from the prior and fit model again (done previously)
  * compute posterior z-scores (z = mu_posterior - theta/sigma_posterior)
  * difference between the posterior mean and the true parameter value divided by the posterior standard deviation. The distribution should have a mean close to 0, otherwise the posterior expectation is a biased estimator.
  
- goal 2: how much uncertainty is removed when updating the prior to the posterior
  * investigate the bias-variance trade-off by computing posterior contraction for each parameter (contraction = 1 - (sigma^2_posterior/sigma^2_prior))
  * if posterior contraction approaches one, the variance of the posterior is negligible compared to the variance of the prior, indicating that the model learned a lot about the parameter of interest. If it is close to zero, very little updating took place.
  
```{r model-sensitivity}
# based on the model specification
# alternative
z           <- (mu_post - sbc_prior) / sqrt(var_post)
contraction <- t(apply(var_post, 1, function(x)  1 - (x/var_prior)))

# scatter plot 1
par(mfrow=c(3, 3))

for(j in 1:npars){
    
  plot(contraction[,j], z[,j],
       xlab = 'Posterior Contraction',
       ylab = 'Z Score',
       main = par_names_post[j],
       xlim = c(-1, 1),
       ylim = c(-4, 4),
       las = 1, bty = 'n',
       pch = 21,
       col = 'grey36', bg = c3)
  points(mean(contraction[,j]), mean(z[, j]),
         col = 'blue',
         pch = 18)
  abline(h=0, lty=3)
    
}
```

## Parameter Recovery and Coverage Probability (Posterior)

### Posterior Expectation

```{r posterior-expectation}
# scatter plot
par(mfrow=c(3, 2))

for(j in 1:npars){
  
  pos_x <- quantile(sbc_prior[,j], 0.80)
  pos_y <- quantile(mu_post[,j], 0.90)
    
  correlation <- cor(sbc_prior[,j], mu_post[,j])
  plot(sbc_prior[,j], mu_post[,j],
       xlab = 'True',
       ylab = 'Estimated',
       main = par_names_post[j],
       las = 1, bty = 'n',
       pch = 21,
       col = 'grey36', bg = c3)
  abline(0, 1)
 # text(pos_x, pos_y, labels=paste0('r =', round(correlation, 2)), font=2, cex = 1.5)
}
```


### Coverage of the Credible Intervals

```{r coverage-ci}

# count the number of times the true value lies within each interval
liesWithin50PercentCI <- data.frame(matrix(nrow = 0, ncol = npars)) 
liesWithin80PercentCI <- data.frame(matrix(nrow = 0, ncol = npars)) 
colnames(liesWithin50PercentCI) <- colnames(liesWithin80PercentCI) <- par_names_post

# c(0.1, 0.9, 0.25, 0.75)
for(i in 1:nsims_ready){
  
  for(j in 1:npars){
    
  true_value <- sbc_prior[i, par_names_prior[j]]
  
  # lies within the 80% interval
  liesWithin80PercentCI[i, j] <- ci_post[[i]][1, par_names_post[j]] < true_value & 
                                 true_value < ci_post[[i]][2, par_names_post[j]] 
  # lies within the 50% interval
  liesWithin50PercentCI[i, j] <- ci_post[[i]][3, par_names_post[j]] < true_value & 
                                 true_value < ci_post[[i]][4, par_names_post[j]] 
  
  }
  
}

# relative frequency and To-Do: 95% CI for binomial proportion 
liesWithin50PercentCI <- apply(liesWithin50PercentCI, 2, mean)
hist(liesWithin50PercentCI)
liesWithin80PercentCI <- apply(liesWithin80PercentCI, 2, mean)
hist(liesWithin80PercentCI)
```



