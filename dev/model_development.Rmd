---
title: "model_development"
author: "AlexandraSarafoglou"
date: "2023-03-28"
output: html_document
---

```{r load-packages-and-functions}
rm(list=ls())
library("rstan")
library("mvtnorm") 
library("papaja")
library("dplyr")
library("magrittr")
library("LaplacesDemon")
library("RColorBrewer")

# helper functions
source('../helper_functions/helpers.R')
source('../helper_functions/init_and_run_short.R')

c1 <- transparentColor(color = "#B3DE69", percent = 50, name = 'colprior')
c2 <- transparentColor(color = "#FCCDE5", percent = 50, name = 'colpost')
c3 <- transparentColor(color = "grey36", percent = 50, name = 'colpost')
```

```{r read-data}
dat_t1 <- read.csv('../data/simulated-all-data-partial_t1.csv', header = TRUE)
dat_t2 <- read.csv('../data/simulated-all-data-partial_t2.csv', header = TRUE)
# dat_t1    <- read.csv('../data/exp1-preprocessed-all.csv', header = TRUE)
dat_t1$qq <- as.factor(dat_t1$qq)
dat_t2$qq <- as.factor(dat_t2$qq)
niter      <- 5000
nwarmup    <- 1000
nchains    <- 2
addparams  <- NULL

```

# Model development

## Run the samplers 

The model development is primarily based on sampling success and theoretical adjustments to the model.
Run all stan models.

```{r set-inits-devel, eval = TRUE}
run_all    <- FALSE
prior      <- TRUE
modelname  <- 'mod_partial_exgauss'

if(run_all){
  
  filename <- list.files('../models/')
  if(prior){
    prior    <- grepl('_prior', filename)
  } else {
    filename <- filename[!grepl('_prior', filename)]
    prior    <- grepl('_prior', filename)
  }
  
  
} else {
  
 filename <- ifelse(prior,  
                    paste0(modelname, '_prior.stan'), 
                    paste0(modelname, '.stan'))
  
}
```

```{r run-models-inits, eval = TRUE}
model_list <- list()

for(i in 1:length(filename)){
#for(i in 1:2){
  
  model_list[[i]] <- rstan::stan_model(file = paste0('../models/', filename[i]))
  
}

```

# Generated Quantities in R 

```{r run-model-in-R, eval = FALSE}
dat_t1    <- read.csv('../data/exp1-preprocessed-all.csv', header = TRUE)
dat_t2    <- dat_t1
dat_t1$qq <- as.factor(dat_t1$qq)
dat_t2$qq <- as.factor(dat_t2$qq)

nsims  <- 3
t      <- 2 
predat <- getDat(dat_t1, dat_t2, prior = prior)
predat$D <- 5
sub_t1 <- predat$sub_t1
sub_t2 <- predat$sub_t2
# timepoint 1
mu_pred_t1 <- matrix(NA, nrow = predat$N_t1, ncol = nsims)
p_pred_t1  <- matrix(NA, nrow = predat$N_t1, ncol = nsims)
y_pred_t1  <- matrix(NA, nrow = predat$N_t1, ncol = nsims)
# timepoint 2
mu_pred_t2 <- matrix(NA, nrow = predat$N_t2, ncol = nsims)
p_pred_t2  <- matrix(NA, nrow = predat$N_t2, ncol = nsims)
y_pred_t2  <- matrix(NA, nrow = predat$N_t2, ncol = nsims)

s_narrow <- 0.1
s_ultra  <- 0.05
s_wide   <- 1
  
for(s in 1:nsims){
  # thresholds:
  # overall
  muprior_b   <- my_rnorm(predat$D, mu=0, sigma=0.1, lower = -0.5, upper = 0.5)
  sdprior_b   <- abs(stats::rt(predat$D, 4, 0.01))
  # variability: 2 narrow, 3 wide
  sdb_narrow_prior <- abs(stats::rt(2, 4, 0.01))
  sdb_wide_prior   <- abs(stats::rt(3, 4, 1))
  sda_prior <- rexp(2, 10)
  
  # correlation between two timepoints (we found moderate correlations)
  # I assume we want to estimate for beta and gamma and each quantifier the correlation separately
  rho_a_raw <- NULL
  rho_b_raw <- NULL
  rho_g_raw <- NULL
  for(d in 1:predat$D){
    
    rho_a_raw[d] <- rbeta(1, 8, 8)  # favor values around 0
    rho_b_raw[d] <- rbeta(1, 10, 6) # favor values between 0 and 0.5
    rho_g_raw[d] <- rbeta(1, 10, 6) # favor values between 0 and 0.5
    
  }
  
  # generated quantities
  rho_a <- (rho_a_raw * 2) - 1 # vagueness: not enough variability to detect correlations
  rho_b <- (rho_b_raw * 2) - 1
  rho_g <- (rho_g_raw * 2) - 1
  
  # expectation: multivariate normals
  mub_more_prior  <- my_mvrnorm(Mu=rep(0, t), Sigma=makeS(rho_b_raw[1], s_narrow), lower = 0, upper = 0.5)  # more than half: narrow
  mub_fewer_prior <- my_mvrnorm(Mu=rep(0, t), Sigma=makeS(rho_b_raw[2], s_narrow), lower = -0.5, upper = 0) # fewer than half: narrow
  mub_most_prior  <- my_mvrnorm(Mu=rep(0, t), Sigma=makeS(rho_b_raw[3], s_wide), lower = 0   , upper = 0.5) # most: wide
  mub_many_prior  <- my_mvrnorm(Mu=rep(0, t), Sigma=makeS(rho_b_raw[4], s_wide), lower = -0.5, upper = 0.5) # many: wide
  mub_few_prior   <- my_mvrnorm(Mu=rep(0, t), Sigma=makeS(rho_b_raw[5], s_wide), lower = -0.5, upper = 0)   # few: wide

  # hyperpriors vagueness: multivariate normals
  mua_prior <- matrix(0, nrow = predat$D, ncol = t)
  for(d in 1:predat$D){
      
      mua_prior[d,] <- rnorm(1, mean=-4, sd=0.1)
      
  }
  # hyperpriors response errors: multivariate normals
  mu_g_prior <- matrix(0, nrow = predat$D, ncol = t)
  for(d in 1:predat$D){
      
      mu_g_prior[d,] <- my_mvrnorm(Mu=rep(0.1, t), Sigma=makeS(rho_g[d], s_narrow), lower = 0, upper = 0.5)
      
  }
  
  alpha_t1_raw_prior <- matrix(0, nrow = predat$I, ncol = predat$D) 
  alpha_t2_raw_prior <- matrix(0, nrow = predat$I, ncol = predat$D)
  
  for(i in 1:predat$I){
    for(d in 1:predat$D){
      
      alpha_raw_prior          <- rmvnorm(1e4, mua_prior[d,], makeS(rho_a[d], s_narrow))
      alpha_t1_raw_prior[i, d] <- alpha_raw_prior[1]
      alpha_t2_raw_prior[i, d] <- alpha_raw_prior[2]
      
    }
  }
  
  # split by timepoints
  beta_more_t1_prior  <- NULL
  beta_fewer_t1_prior <- NULL
  beta_most_t1_prior  <- NULL
  beta_many_t1_prior  <- NULL
  beta_few_t1_prior   <- NULL
  beta_more_t2_prior  <- NULL
  beta_fewer_t2_prior <- NULL
  beta_most_t2_prior  <- NULL
  beta_many_t2_prior  <- NULL
  beta_few_t2_prior   <- NULL
  alpha_t1_prior      <- matrix(0.001, nrow = predat$I, ncol = predat$D)
  alpha_t2_prior      <- matrix(0.001, nrow = predat$I, ncol = predat$D)
  gamma_t1_prior      <- matrix(0.2, nrow = predat$I, ncol = predat$D)
  gamma_t2_prior      <- matrix(0.2, nrow = predat$I, ncol = predat$D)
  
  # transform parameters
  for(i in 1:predat$I){
    for(d in 1:predat$D){
      
      # # thresholds: timepoint 1 and 2
      beta_more_t1_prior[i]  <- my_rnorm(1, mub_more_prior[1] , 0.1, lower = 0.  , upper = 0.5)
      beta_fewer_t1_prior[i] <- my_rnorm(1, mub_fewer_prior[1], 0.1, lower = -0.5, upper = 0)
      beta_most_t1_prior[i]  <- my_rnorm(1, mub_most_prior[1] , 0.5, lower = 0   , upper = 0.5)
      beta_many_t1_prior[i]  <- my_rnorm(1, mub_many_prior[1] , 0.5, lower = -0.5, upper = 0.5)
      beta_few_t1_prior[i]   <- my_rnorm(1, mub_few_prior[1]  , 0.5, lower = -0.5, upper = 0)
      beta_more_t2_prior[i]  <- my_rnorm(1, mub_more_prior[2] , 0.1, lower = 0.  , upper = 0.5)
      beta_fewer_t2_prior[i] <- my_rnorm(1, mub_fewer_prior[2], 0.1, lower = -0.5, upper = 0)
      beta_most_t2_prior[i]  <- my_rnorm(1, mub_most_prior[2] , 0.5, lower = 0   , upper = 0.5)
      beta_many_t2_prior[i]  <- my_rnorm(1, mub_many_prior[2] , 0.5, lower = -0.5, upper = 0.5)
      beta_few_t2_prior[i]   <- my_rnorm(1, mub_few_prior[2]  , 0.5, lower = -0.5, upper = 0)
      # vagueness: timepoint 1 and 2
      alpha_t1_prior[i, d]    <- exp(alpha_t1_raw_prior[i, d])
      alpha_t2_prior[i, d]    <- exp(alpha_t2_raw_prior[i, d])
      # # response error: timepoint 1 and 2
      gamma_t1_prior[i, d]    <- my_rnorm(1, mu_g_prior[d, 1], 0.1, lower = 0, upper = 0.5)
      gamma_t2_prior[i, d]    <- my_rnorm(1, mu_g_prior[d, 2], 0.1, lower = 0, upper = 0.5)
      
    }
  }
  
  # timepoint 1
  for (n in 1:predat$N_t1) {
    
    mu_pred_t1[n, s] <- predat$more_t1[n] * (predat$cperc_t1[n] - beta_more_t1_prior[sub_t1[n]]) / alpha_t1_prior[sub_t1[n], 1] +
      predat$fewer_t1[n]  * (predat$cperc_t1[n] - beta_fewer_t1_prior[sub_t1[n]]) / alpha_t1_prior[sub_t1[n], 2] +
      predat$most_t1[n]  * (predat$cperc_t1[n] - beta_most_t1_prior[sub_t1[n]]) / alpha_t1_prior[sub_t1[n], 3] +
      predat$many_t1[n]  * (predat$cperc_t1[n] - beta_many_t1_prior[sub_t1[n]]) / alpha_t1_prior[sub_t1[n], 4] +
      predat$few_t1[n]  * (predat$cperc_t1[n] - beta_few_t1_prior[sub_t1[n]]) / alpha_t1_prior[sub_t1[n], 5] 
    
    p_pred_t1[n, s] <- predat$more_t1[n] * gamma_t1_prior[sub_t1[n], 1] +
      predat$fewer_t1[n] * gamma_t1_prior[sub_t1[n], 2] +
      predat$most_t1[n] * gamma_t1_prior[sub_t1[n], 3] +
      predat$many_t1[n] * gamma_t1_prior[sub_t1[n], 4] +
      predat$few_t1[n] * gamma_t1_prior[sub_t1[n], 5] +
      (1 - 2 * (predat$more_t1[n] * gamma_t1_prior[sub_t1[n], 1] +
                  predat$fewer_t1[n] * gamma_t1_prior[sub_t1[n], 2] +
                  predat$most_t1[n] * gamma_t1_prior[sub_t1[n], 3] +
                  predat$many_t1[n] * gamma_t1_prior[sub_t1[n], 4] +
                  predat$few_t1[n] * gamma_t1_prior[sub_t1[n], 5])) *
    (1 - exp(-pmax(mu_pred_t1[n, s], .001)))
    
    y_pred_t1[n, s] <- rbinom(1, 1, p_pred_t1[n, s])
    
  }
  
  # timepoint 2
  for (n in 1:predat$N_t2) {
    
    mu_pred_t2[n, s] <- predat$more_t2[n] * (predat$cperc_t2[n] - beta_more_t2_prior[sub_t2[n]]) / alpha_t2_prior[sub_t2[n], 1] +
      predat$fewer_t2[n]  * (predat$cperc_t2[n] - beta_fewer_t2_prior[sub_t2[n]]) / alpha_t2_prior[sub_t2[n], 2] +
      predat$most_t2[n]  * (predat$cperc_t2[n] - beta_most_t2_prior[sub_t2[n]]) / alpha_t2_prior[sub_t2[n], 3] +
      predat$many_t2[n]  * (predat$cperc_t2[n] - beta_many_t2_prior[sub_t2[n]]) / alpha_t2_prior[sub_t2[n], 4] +
      predat$few_t2[n]  * (predat$cperc_t2[n] - beta_few_t2_prior[sub_t2[n]]) / alpha_t2_prior[sub_t2[n], 5] 
    
    p_pred_t2[n, s] <- predat$more_t2[n] * gamma_t2_prior[sub_t2[n], 1] +
      predat$fewer_t2[n] * gamma_t2_prior[sub_t2[n], 2] +
      predat$most_t2[n] * gamma_t2_prior[sub_t2[n], 3] +
      predat$many_t2[n] * gamma_t2_prior[sub_t2[n], 4] +
      predat$few_t2[n] * gamma_t2_prior[sub_t2[n], 5] +
      (1 - 2 * (predat$more_t2[n] * gamma_t2_prior[sub_t2[n], 1] +
                  predat$fewer_t2[n] * gamma_t2_prior[sub_t2[n], 2] +
                  predat$most_t2[n] * gamma_t2_prior[sub_t2[n], 3] +
                  predat$many_t2[n] * gamma_t2_prior[sub_t2[n], 4] +
                  predat$few_t2[n] * gamma_t2_prior[sub_t2[n], 5])) *
    (1 - exp(-pmax(mu_pred_t1[n, s], .001)))
    
    y_pred_t2[n, s] <- rbinom(1, 1, p_pred_t2[n, s])
    
  }

}

vec <- c(0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100)
# analyze response patterns: t1
predat$qq     <- as.numeric(dat_t1$qq)
predat$cperc_t1       <- predat$cperc_t1 * 100 + 50
predat$percent_cat_t1 <- cut(predat$cperc_t1,
                             breaks=vec,
                             labels=as.character((vec + 5)[-length(vec)]))
# analyze response patterns: t2
predat$cperc_t2       <- predat$cperc_t2 * 100 + 50
predat$percent_cat_t2 <- cut(predat$cperc_t2,
                             breaks=vec,
                             labels=as.character((vec + 5)[-length(vec)]))
# need to fix predat
analyzePredictedResponsesR(y_pred_t1, predat, testtime = 't1')
analyzePredictedResponsesR(y_pred_t2, predat, testtime = 't2')
analyzePredictedResponsesR(y_pred_t1, predat, individual = TRUE, testtime = 't1')
analyzePredictedResponsesR(y_pred_t2, predat, individual = TRUE, testtime = 't2')

# correlations between t1 and t2: alpha, beta, and gamma parameters?
  # plot(rowMeans(p_pred_t1), rowMeans(p_pred_t2))
  # cor(rowMeans(p_pred_t1), rowMeans(p_pred_t2))
```

Conclusion:
- gamma needs to be restricted to values smaller than 0.5 (as it flips the distribution otherwise)
- beta needs to range between -0.5 and 0.5 to be on the same scale as cperc
- alpha: draw from normal distribution, then exponentiate, priors can be reused

```{r initialize_b_thresholds, eval = TRUE}
# 1 = FewerThanHalf
# 2 = MoreThanHalf
# 3 = Few
# 4 = Most
# 5 = Many
b_min          <- c(-0.5, 0, -0.5, 0, -0.5)
b_max          <- c(0, 0.5, 0, 0.5, 0.5)
mu_b_sd        <- c(0.1, 0.1, 0.5, 0.5, 0.5)
invgamma_mub_b <- c(0.5, 0.5, 1, 1, 1)
quant          <- c('Fewer', 'More', 'Few', 'Most', 'Many')
```

```{r try_out_model, eval = FALSE}
# split up the data for the simulation
dat_t1       <- dat
predat       <- getDat(dat_t1, dat_t2, prior = prior)
predat$D     <- 1
predat$T     <- 2
predat$b_min <- -0.5
predat$b_max <- 0

logmodfit <- myRunner(predat
                      , prior = FALSE
                      , addparams = addparams
                      , mod = model_list[[1]]
                      , iter = niter
                      , warmup = nwarmup
                      , control = list(adapt_delta = .99, max_treedepth = 17)
                      , nchains = nchains)

print(names(logmodfit))
pairs(logmodfit, pars = c("sigma_mu_a"))
pairs(logmodfit, pars = c("mu_b", "mu_log_a", "mu_g"))
pairs(logmodfit, pars = c("rho_raw_b", "rho_raw_a", "rho_raw_g"))

samples_processed <- removeDivergentTransitions(logmodfit, nchains = nchains)

list_of_draws  <- rstan::extract(logmodfit)
array_of_draws <- as.array(logmodfit)
posterior2     <- rstan::extract(logmodfit, inc_warmup = FALSE, permuted = FALSE)
sampler_params <- rstan::get_sampler_params(logmodfit, inc_warmup = FALSE)
Rhat <- sapply(list_of_draws, function(x) rstan::Rhat(as.matrix(x)))
Rhat

samples_processed <- removeDivergentTransitions(logmodfit, nchains = nchains)
samples_processed <- samples_processed[seq(from=1, to=nrow(samples_processed), by = 50),]
pred_names_t1     <- colnames(samples_processed)[grepl('y_t1', colnames(samples_processed))]
pred_names_t2     <- colnames(samples_processed)[grepl('y_t2', colnames(samples_processed))]
pred_responses_t1 <- samples_processed[, pred_names_t1]
pred_responses_t1 <- t(pred_responses_t1)
pred_responses_t2 <- samples_processed[, pred_names_t2]
pred_responses_t2 <- t(pred_responses_t2)
# analyze response patterns: t1
vec <- c(0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100)
predat$qq     <- as.numeric(dat_t1$qq)
predat$qq     <- predat$qq[as.logical(predat$many_t1)]
predat$cperc_t1       <- predat$cperc_t1 * 100 + 50
predat$percent_cat_t1 <- cut(predat$cperc_t1,
                             breaks=vec,
                             labels=as.character((vec + 5)[-length(vec)]))
# analyze response patterns: t2
predat$cperc_t2       <- predat$cperc_t2 * 100 + 50
predat$percent_cat_t2 <- cut(predat$cperc_t2,
                             breaks=vec,
                             labels=as.character((vec + 5)[-length(vec)]))
analyzePredictedResponsesR(y_pred=pred_responses_t1, predat, testtime = 't1', individual = TRUE)
analyzePredictedResponsesR(y_pred=pred_responses_t2, predat, testtime = 't2', individual = TRUE)
```


```{r run-model, eval = FALSE}
for(i in 1:length(quant)){
  
  predat                <- getDat(dat_t1, dat_t2, prior = prior)
  predat$b_min          <- b_min[i]
  predat$b_max          <- b_max[i]
  predat$mu_b_sd        <- mu_b_sd[i]
  predat$invgamma_mub_b <- invgamma_mub_b[i]
  
  # save(predat, file = paste0('../output/predat_sim_', gsub('stan', 'rda', filename[i])))
  logmodfit <- myRunner(predat
                      , prior = prior
                      , addparams = addparams
                      , mod = model_list[[1]]
                      , iter = niter
                      , warmup = nwarmup
                      , control = list(adapt_delta = .99, max_treedepth = 17)
                      , nchains = nchains)
  save(logmodfit, file = paste0('../output/',quant[i], '_', gsub('stan', 'rda', filename)))

# if it is a prior distribution
if(prior){
  
  # (1) remove divergent transitions
  samples_processed <- removeDivergentTransitions(logmodfit, nchains = nchains)
  
  # (2) thin the chains
  samples_processed <- samples_processed[seq(from=1, to=nrow(samples_processed), by = 50),]
  
  # (3) export parameter values from priors
  pred_names <- colnames(samples_processed)[!grepl('pred', colnames(samples_processed)) & 
                                            !grepl('lp__', colnames(samples_processed))]
  write.csv2(samples_processed[, pred_names],
             paste0('../output/prior_pred_all_', quant[i], '_', modelname, '.csv'),
             row.names = FALSE)
  
  # (4) export predicted responses from priors
  pred_names     <- colnames(samples_processed)[grepl('y_pred', colnames(samples_processed))]
  pred_responses <- samples_processed[, pred_names]
  pred_responses_orig <- pred_responses
  colnames(pred_responses_orig) <- paste0('y_pred_', 1:length(pred_names))
  write.csv2(pred_responses_orig,
             paste0('../output/prior_pred_', quant[i], '_', modelname, '.csv'),
             row.names = FALSE)
  
}
}

```


# Model validation 

```{r set-inits-val, eval = TRUE}
prior     <- FALSE
modelname <- 'mod_partial_exgauss'
filename  <- ifelse(prior,  
                    paste0(modelname, '_prior.rda'), 
                    paste0(modelname, '.rda'))
```

## Inspect predicted response patterns

```{r inspect-predicted-responses, eval = FALSE}
predat    <- getDat(dat_t1, dat_t2, prior = prior)
vec       <- c(0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100)
predat$qq <- as.numeric(dat_t1$qq)
predat$cperc_t1       <- predat$cperc_t1 * 100 + 50
predat$percent_cat_t1 <- cut(predat$cperc_t1,
                             breaks=vec,
                             labels=as.character((vec + 5)[-length(vec)]))
predat$cperc_t2       <- predat$cperc_t2 * 100 + 50
predat$percent_cat_t2 <- cut(predat$cperc_t2,
                             breaks=vec,
                             labels=as.character((vec + 5)[-length(vec)]))

y_pred_dat <- list()

for(i in 1:length(quant)){
  y_pred_dat[[i]] <- t(read.csv(paste0('../output/prior_pred_', quant[i], '_', modelname, '.csv'),
                       header = TRUE, sep = ';'))
}

for(i in 1:length(quant)){
  predat$quant_name <- quant[i]
  analyzePredictedResponsesR(y_pred=y_pred_dat[[i]][1:2400,], predat, testtime = 't1', individual = TRUE)
  analyzePredictedResponsesR(y_pred=y_pred_dat[[i]][2401:4800,], predat, testtime = 't2', individual = TRUE)
}
```

## Inspect parameter distribution and convergence

```{r load-post-samples}
source('../helper_functions/load_samples.R')
load(paste0('../output/predat_sim_', filename))
```

```{r check-model-convergence}
parameters      <- c('alpha', 'beta', 'gamma')
hyperparameters <- c('sigmaalpha', 'nu', 'mug', 'mub', 'etag', 'etab')
# hyperparameters <- c('sigma', 'delta')

convergence_diagnostics <- checkModelConvergence(list_of_draws, parameters, hyperparameters, prior)
convergence_diagnostics$reached_convergence
convergence_diagnostics$Rhat_params
convergence_diagnostics$plot_list[[1]]
convergence_diagnostics$plot_list[[2]]
convergence_diagnostics$plot_list[[3]]
```

```{r remove-divergent-transitions}
# remove divergent transitions before any further analysis
samples_processed <- removeDivergentTransitions(logmodfit, nchains = nchains)
# To-Do: how many divergent transitions were removed?
```

```{r check-parameter-distributions}
pid <- 4
pars <- c('alpha', 'beta', 'gamma')
if(prior) pars <- paste0(pars, 'prior')

# Relevant plots: differences in quantifiers for the alpha and beta parameters
par(mfrow=c(2,2))
for(i in seq_along(pars)){
  
  drawDistribution(pid, quantifier = 'More than half', parameter = pars[i])
  drawDistribution(pid, quantifier = 'Fewer than half', parameter = pars[i])
  
}
```

## Prior and posterior predictive checks

- Posterior predictive: match between observed and estimated response patterns
- Prior predictive
  * monotonic increase in the probability to respond 'true'
  * small response error: 0-20 and 80 - 100, 80% should be in accordance for the expectations
  * vagueness: at threshold parameter: maximum uncertainty, about 50% probability to say 'true'
- To-Do: individual response patterns generated from the prior predictives make no sense

```{r inspect-response-patterns}
if(max(predat$cperc) < 1) predat$cperc <- predat$cperc * 100 + 50
predat$percent_cat <- cut(predat$cperc,
                      breaks=c(0, 20, 40, 60, 80, 100),
                      labels=c("1-20","21-40","41-60", "61-80", "81-100"))
predat$qq    <- ifelse(predat$fewer == 1, 1, 2)

predResponses <- analyzePredictedResponses(samples_processed, predat,
                                           prior, modelname, hyperparameters)

# prior predict: random row
# posterior predict: aggregated over all
if(prior){
  
  predResponses$monotonic_increase
  predResponses$small_response_error
  predResponses$constrained_vagueness
      
}

par(mfrow=c(2,2))
for(i in 1:length(predResponses$pred_plot)){
 replayPlot(predResponses$pred_plot[[i]])
}
```

### Sampling characteristics

Convergence diagnostics, sampling properties

## Computational faithfulness

Simulation based calibration.
- step 1: simulate data from the prior distribution
- step 2: fit the model to the simulated data

```{r initialize-simulation-study}
d = 1
nsims_target   <- 100
nsims_ready    <- length(list.files('../output/', pattern=paste0("sim_mu_post_", modelname, '_', quant[d]), full.names = TRUE, recursive = TRUE))
nsims          <- nsims_target - nsims_ready

pred_responses <- read.csv2(paste0('../output/prior_pred_', quant[d], '_', modelname, '.csv'),
                             header = TRUE, sep =';')
# initialize quantities required for model diagnostics
  # par_names       <- c("mu_g", "mu_b", "mu_log_a", 
  #                      "rho_raw_a", "rho_raw_b", "rho_raw_g", 
  #                      "log_a", "b", "g",
  #                      "sigma_mu_a", "sigma_mu_b", "sigma_mu_g") 
  # select relevant columns
  # par_names_exact <- colnames(sbc_post)[!grepl('pred', colnames(sbc_post)) &
  #                                       !grepl('lp__', colnames(sbc_post))]
  # only hyperparameters
  # par_names_prior <- c(paste0(par_names, 'prior.1.'), paste0(par_names, 'prior.2.'))
  # par_names_post  <- c(paste0(par_names, '[1]'), paste0(par_names, '[2]'))

model_list     <- rstan::stan_model(file = paste0('../models/', modelname, '.stan'))
# dat_prior      <- dat

# load prior parameters samples (divergent transitions already removed)
for(d in 1:length(quant)){
  
  if (d == 1) {
    
    sbc_prior <- read.csv2(paste0('../output/prior_pred_all_', quant[d], '_', modelname, '.csv'),
                       header = TRUE, sep =';')[1:nsims_target,]    
  } else {
    
    sbc_prior_new <- read.csv2(paste0('../output/prior_pred_all_', quant[d], '_', modelname, '.csv'),
                       header = TRUE, sep =';') [1:nsims_target,]  
    sbc_prior <- rbind(sbc_prior, sbc_prior_new)
    
  }

}

mu_prior           <- apply(sbc_prior, 2, mean)
par_names_post <- par_names_prior <- colnames(sbc_prior)
colnames(mu_prior) <- colnames(par_names_prior)
var_prior          <- apply(sbc_prior, 2, var) 
```


```{r run-simulation-study, eval = FALSE}
# # compute necessary quantities
#   rank           <- data.frame(matrix(nrow = 0, ncol = length(par_names_post))) 
#   mu_post        <- data.frame(matrix(nrow = 0, ncol = length(par_names_post))) 
#   var_post       <- data.frame(matrix(nrow = 0, ncol = length(par_names_post))) 
#   ci_post        <- list()
#   colnames(rank) <- colnames(mu_post) <- colnames(var_post) <- par_names_post
niter   <- 1500
nwarmup <- 500

for(i in 1:nsims){
  
  predat         <- getDat(dat_t1, dat_t2, prior = FALSE)
  predat$y_t1    <- as.integer(pred_responses[nsims_ready + i, 1:2400])
  predat$y_t2    <- as.integer(pred_responses[nsims_ready + i, 2401:4800])
  predat$b_min          <- b_min[d]
  predat$b_max          <- b_max[d]
  predat$mu_b_sd        <- mu_b_sd[d]
  #predat$invgamma_mub_b <- invgamma_mub_b[d]
  
  save(predat, file = paste0('../output/predat_sbc_', quant[d], nsims_ready + i, '_',  modelname, '.rda'))
  logmodfit <- myRunner(predat
                      , prior = FALSE
                      , addparams = addparams
                      , mod = model_list
                      , iter = niter
                      , warmup = nwarmup
                      , control = list(adapt_delta = .97, max_treedepth = 14)
                      , nchains = nchains)

  # remove divergent transitions
  sbc_post <- removeDivergentTransitions(logmodfit, nchains = nchains)
  sbc_post <- sbc_post[, -which(colnames(sbc_post) == 'lp__')]
  par_names_post  <- colnames(sbc_post) 
  par_names_prior <- colnames(sbc_prior)
  # compute the posterior means and variances
  mu_post  <- apply(sbc_post, 2, mean)
  var_post <- apply(sbc_post, 2, var) 
  # compute the 50% and 80% credible intervals
  ci_post  <- apply(sbc_post, 2, function(x) quantile(x, c(0.1, 0.9, 0.25, 0.75)))
  # thin the chains
  sbc_post <- sbc_post[seq(from=1, to=nrow(sbc_post), by = 20),]
  
  rank <- data.frame(matrix(nrow = 1, ncol = ncol(sbc_post)))
  colnames(rank) <- par_names_post
  
  # loop over parameters
  for (j in 1:length(par_names_post)) {
    
    # count the posterior samples that are smaller than the draw from the prior
    rank[, j] <- sum(sbc_post[, par_names_post[j]] < sbc_prior[nsims_ready + i, par_names_prior[j]])
    
  }
  
  # export results
  write.csv2(mu_post, paste0('../output/sim_mu_post_', modelname, '_', quant[d],nsims_ready + i, '.csv'), row.names = FALSE)
  write.csv2(var_post, paste0('../output/sim_var_post_', modelname, '_', quant[d],nsims_ready + i, '.csv'), row.names = FALSE)
  write.csv2(ci_post, paste0('../output/sim_ci_post_', modelname, '_', quant[d],nsims_ready + i, '.csv'), row.names = FALSE)
  write.csv2(rank, paste0('../output/sim_rank_', modelname, '_', quant[d],nsims_ready + i, '.csv'), row.names = FALSE)
  
}

```


```{r load-sim-files}
npars          <- ncol(sbc_prior)

# compute necessary quantities
  rank           <- data.frame(matrix(nrow = 0, ncol = npars))
  mu_post        <- data.frame(matrix(nrow = 0, ncol = npars))
  var_post       <- data.frame(matrix(nrow = 0, ncol = npars))
  ci_post        <- list()
  colnames(rank) <- colnames(mu_post) <- colnames(var_post) <- par_names_post

  
for(d in 1:5){
  for(i in 1:100){
    
     mu_post[nrow(mu_post) + 1,] <-  t(read.csv2(paste0('../output/sim_mu_post_', modelname, '_', quant[d], i, '.csv'),
                             header = TRUE, sep =';'))
     var_post[nrow(var_post) + 1,] <-  t(read.csv2(paste0('../output/sim_var_post_', modelname, '_', quant[d], i, '.csv'),
                             header = TRUE, sep =';'))
     rank[nrow(rank) + 1,] <-  t(read.csv2(paste0('../output/sim_rank_', modelname, '_', quant[d], i, '.csv'),
                             header = TRUE, sep =';'))
      ci_post[[length(ci_post) + 1]] <-  read.csv2(paste0('../output/sim_ci_post_', modelname, '_', quant[d], i, '.csv'),
                             header = TRUE, sep =';')
  }
}

```


- step 3: compute the rank statistics of the prior estimates with the 'posterior' estimates
- we are interested in the hyperlevel parameters
- each posterior simulation should return only one value, that is, the number of samples smaller than the prior sample

```{r sbc}
# plot the historgrams
par(mfrow=c(2, 3))

for(j in 1:npars){
    
  hist(rank[, j], las = 1, 
       main = par_names_post[j], 
       xlab = 'Rank')
    
}

# # to visualize the overlap between the two distributions
# par(mfrow=c(2, 2))
# 
# for(j in 1:length(par_names_prior)){
#   
#   hgA      <- hist(sbc_prior[, par_names_prior[j]], plot = FALSE) 
#   hgB      <- hist(sbc_post[, par_names_post[j]] , plot = FALSE) 
#   ylim_max <- max(signif(hgA$density * 1.1, 3), signif(hgB$density * 1.1, 3))
#   
#   hist(sbc_prior[, par_names_prior[j]], las = 1, col = c1, xlab = par_names_exact[j], 
#        main = '', ylim = c(0, ylim_max), freq = FALSE)
#   hist(sbc_post[, par_names_post[j]], las = 1, col = c2, freq = FALSE, add = TRUE)
#   
# }

```


## Model sensitivity

- goal 1: how well does the posterior mean match the true data generating process
  * sample from the prior of the model (done previously)
  * simulate data from the prior and fit model again (done previously)
  * compute posterior z-scores (z = mu_posterior - theta/sigma_posterior)
  * difference between the posterior mean and the true parameter value divided by the posterior standard deviation. The distribution should have a mean close to 0, otherwise the posterior expectation is a biased estimator.
  
- goal 2: how much uncertainty is removed when updating the prior to the posterior
  * investigate the bias-variance trade-off by computing posterior contraction for each parameter (contraction = 1 - (sigma^2_posterior/sigma^2_prior))
  * if posterior contraction approaches one, the variance of the posterior is negligible compared to the variance of the prior, indicating that the model learned a lot about the parameter of interest. If it is close to zero, very little updating took place.
  
```{r model-sensitivity}
# based on the model specification
z <- mu_post - sbc_prior
contraction <- var_post - var_prior
z <- t(apply(mu_post, 1, function(x) x - mu_prior))/sqrt(var_post)
z <- t(apply(mu_post, 1, function(x) x - mu_prior))
contraction <- t(apply(var_post, 1, function(x) x - var_prior))
contraction <- t(apply(var_post, 1, function(x)  1 - (x/var_prior)))

# scatter plot 1
par(mfrow=c(3, 3))

for(j in 1:npars){
    
  plot(contraction[,j], z[,j],
       xlab = 'Var_post - Var_prior',
       ylab = 'Mu_post - Mu_prior',
       main = par_names_post[j],
       xlim = c(-1, 1),
       ylim = c(-4, 4),
       las = 1, bty = 'n',
       pch = 21,
       col = 'grey36', bg = c3)
  points(mean(contraction[,j]), mean(z[, j]),
         col = 'blue',
         pch = 18)
  abline(h=0, lty=3)
    
}

# look at specific parameters
rhos <- c(80, 81, 82)

rhos <- c(19, 20, 37, 38, 64, 65)
rhos <- which(grepl('^mu', par_names_post))
par(mfrow=c(3, 2))

for(j in 1:length(rhos)){
  
    plot(contraction[,rhos[j]], z[,rhos[j]],
       xlab = 'Posterior contraction',
       ylab = 'Posterior z-score',
       main = par_names_post[rhos[j]],
       xlim = c(-1, 1),
       ylim = c(-4, 4),
       las = 1, bty = 'n',
       pch = 21,
       col = 'grey36', bg = c3)
  points(mean(contraction[,rhos[j]]), mean(z[,rhos[j]]),
         col = 'blue',
         pch = 18)
  abline(h=0, lty=3)
  
}

```

## Parameter Recovery and Coverage Probability (Posterior)

### Posterior Expectation

```{r posterior-expectation}
# scatter plot
par(mfrow=c(3, 2))

for(j in 1:npars){
  
  pos_x <- quantile(sbc_prior[,j], 0.80)
  pos_y <- quantile(mu_post[,j], 0.90)
    
  correlation <- cor(sbc_prior[,j], mu_post[,j])
  plot(sbc_prior[,j], mu_post[,j],
       xlab = 'True',
       ylab = 'Estimated',
       main = par_names_post[j],
       las = 1, bty = 'n',
       pch = 21,
       col = 'grey36', bg = c3)
  abline(0, 1)
 # text(pos_x, pos_y, labels=paste0('r =', round(correlation, 2)), font=2, cex = 1.5)
}

# only show the correlations
par(mfrow=c(3, 1))
rhos <- c(80, 81, 82)
for(j in 1:length(rhos)){
  
  pos_x <- quantile(sbc_prior[,rhos[j]], 0.80)
  pos_y <- quantile(mu_post[,rhos[j]], 0.20)
  correlation <- cor(sbc_prior[,rhos[j]], mu_post[,rhos[j]])
  plot(sbc_prior[,rhos[j]], mu_post[,rhos[j]],
       xlab = 'True',
       ylab = 'Estimated',
       main = par_names_post[rhos[j]],
       las = 1, bty = 'n',
       pch = 21,
      # xlim = c(-1, 1),
      # ylim = c(-1, 1),
       col = 'grey36', bg = c3)
  #abline(0, 1)
  text(pos_x, pos_y, labels=paste0('r =', round(correlation, 2)), font=2, cex = 1.5)
}

```


### Coverage of the Credible Intervals

```{r coverage-ci}

# count the number of times the true value lies within each interval
liesWithin50PercentCI <- data.frame(matrix(nrow = 0, ncol = npars)) 
liesWithin80PercentCI <- data.frame(matrix(nrow = 0, ncol = npars)) 
colnames(liesWithin50PercentCI) <- colnames(liesWithin80PercentCI) <- par_names_post

# c(0.1, 0.9, 0.25, 0.75)
for(i in 1:nsims_ready){
  
  for(j in 1:npars){
    
  true_value <- sbc_prior[i, par_names_prior[j]]
  
  # lies within the 80% interval
  liesWithin80PercentCI[i, j] <- ci_post[[i]][1, par_names_post[j]] < true_value & 
                                 true_value < ci_post[[i]][2, par_names_post[j]] 
  # lies within the 50% interval
  liesWithin50PercentCI[i, j] <- ci_post[[i]][3, par_names_post[j]] < true_value & 
                                 true_value < ci_post[[i]][4, par_names_post[j]] 
  
  }
  
}

# relative frequency and To-Do: 95% CI for binomial proportion 
liesWithin50PercentCI <- apply(liesWithin50PercentCI, 2, mean)
liesWithin80PercentCI <- apply(liesWithin80PercentCI, 2, mean)
```



